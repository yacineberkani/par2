{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9fea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from langdetect import detect\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51bd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_mots_cles_fr(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en français.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    requete_utilisateur = requete.lower()\n",
    "    doc = nlp(requete_utilisateur)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" and token.dep_ not in [\"ROOT\",\"obj\"]) or \\\n",
    "           (token.pos_ == \"ADV\" ) or \\\n",
    "           (token.pos_ == \"PRON\" and token.dep_ != \"dep\") or \\\n",
    "           (token.dep_ == \"ROOT\" and token.pos_ not in [\"NOUN\",\"VERB\"]) or \\\n",
    "           (token.pos_ == \"ADJ\" ) or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ in [\"nmod\",\"punct\"]):\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337b4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_mots_cles_en(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en anglais.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(requete)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" ) or  (token.dep_ == \"punct\") or \\\n",
    "           (token.pos_ == \"ADV\") or (token.dep_ == \"pobj\") or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ != \"dep\") or \\\n",
    "           (token.pos_ == \"ADJ\" ) :\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e14b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Fonction pour détecter la langue d'un texte donné.\n",
    "    \n",
    "    Arguments :\n",
    "        - text : Le texte à analyser\n",
    "        \n",
    "    Returns :\n",
    "        - 'français' si la langue détectée est le français, sinon 'anglais'\n",
    "    \"\"\"\n",
    "    lang = detect(text)\n",
    "    if lang == 'fr':\n",
    "        return 'français'\n",
    "    else:\n",
    "        return 'anglais'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mot(motcle):\n",
    "    \"\"\"\n",
    "    Fonction pour formater un mot-clé pour une utilisation dans une URL de recherche.\n",
    "    \n",
    "    Arguments :\n",
    "        - motcle : Le mot-clé à formater\n",
    "        \n",
    "    Returns :\n",
    "        - format_motcle : Le mot-clé formaté pour une URL de recherche\n",
    "    \"\"\"\n",
    "    if ' ' in motcle:\n",
    "        format_motcle = motcle.replace(' ', '+')\n",
    "    else:\n",
    "        format_motcle = motcle\n",
    "    return format_motcle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f8be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Demander à l'utilisateur le sujet de recherche\n",
    "    requete_utilisateur = str(input('Veuillez saisir votre requête : '))\n",
    "\n",
    "    # Détection de la langue de la requête\n",
    "    langue = detect_language(requete_utilisateur)\n",
    "\n",
    "    # Sélection de la fonction d'extraction des mots-clés en fonction de la langue détectée\n",
    "    if langue == 'français':\n",
    "        mots_cles_format = extraire_mots_cles_fr(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Français\")\n",
    "    else:\n",
    "        mots_cles_format = extraire_mots_cles_en(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Anglais\")\n",
    "\n",
    "    # Formatage des mots-clés pour une utilisation dans une URL de recherche\n",
    "    search_query = format_mot(mots_cles_format)\n",
    "\n",
    "    # Demander à l'utilisateur le nombre de pages à parcourir\n",
    "    num_pages = int(input(\"Entrez le nombre de pages à parcourir : \"))\n",
    "\n",
    "    # Créer le nom du dossier de destination\n",
    "    folder_name = f\"{search_query.replace('+', '-')}-{num_pages}\"\n",
    "\n",
    "    # Créer le dossier s'il n'existe pas\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Initialiser le navigateur Chrome en mode sans tête\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Parcourir les pages demandées\n",
    "    for page_num in range(1, num_pages+1):\n",
    "        print(f\"\\nPage {page_num}\")\n",
    "        # Ouvrir la page de recherche de Semantic Scholar\n",
    "        driver.get(f\"https://www.semanticscholar.org/search?q={search_query}&sort=relevance&page={page_num}\")\n",
    "\n",
    "        # Attendre quelques secondes pour permettre au contenu de charger\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Obtenir le contenu de la page\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Analyser le contenu HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Trouver tous les liens vers les PDF\n",
    "        pdf_links = soup.select('a[href$=\".pdf\"]')\n",
    "\n",
    "        # Télécharger les PDF et les enregistrer dans le dossier \"articles\"\n",
    "        for link in pdf_links:\n",
    "            pdf_url = link[\"href\"]\n",
    "            print(\"Téléchargement du PDF:\", pdf_url)\n",
    "            try:\n",
    "                response = requests.get(pdf_url)\n",
    "                # Générer un nom de fichier unique pour chaque PDF\n",
    "                file_name = f\"article_{time.time()}.pdf\"\n",
    "                # Enregistrer le PDF dans le dossier \"articles\"\n",
    "                with open(os.path.join(folder_name, file_name), \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Enregistré sous: {folder_name}/{file_name}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "\n",
    "    # Fermer le navigateur\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041d4d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez saisir votre requête : I'm looking for articles on NLP\n",
      "Langue détectée ==> Anglais\n",
      "Entrez le nombre de pages à parcourir : 4\n",
      "\n",
      "Page 1\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/1902.00751.pdf\n",
      "Enregistré sous: NLP-4/article_1713797715.27706.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1355.pdf\n",
      "Enregistré sous: NLP-4/article_1713797715.766979.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2005.11401.pdf\n",
      "Enregistré sous: NLP-4/article_1713797715.93277.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.340.pdf\n",
      "Enregistré sous: NLP-4/article_1713797717.1812868.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.442.pdf\n",
      "Enregistré sous: NLP-4/article_1713797717.746401.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.168.pdf\n",
      "Enregistré sous: NLP-4/article_1713797718.235768.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.findings-acl.84.pdf\n",
      "Enregistré sous: NLP-4/article_1713797719.2912269.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1452.pdf\n",
      "Enregistré sous: NLP-4/article_1713797719.789164.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n",
      "Enregistré sous: NLP-4/article_1713797720.2811031.pdf\n",
      "\n",
      "Page 2\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-4/article_1713797726.421663.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-4/article_1713797726.902575.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.560.pdf\n",
      "Enregistré sous: NLP-4/article_1713797727.399324.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.386.pdf\n",
      "Enregistré sous: NLP-4/article_1713797727.8866901.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.insights-1.1.pdf\n",
      "Enregistré sous: NLP-4/article_1713797728.7349172.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.324.pdf\n",
      "Enregistré sous: NLP-4/article_1713797729.223244.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.500.pdf\n",
      "Enregistré sous: NLP-4/article_1713797730.2714722.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.482.pdf\n",
      "Enregistré sous: NLP-4/article_1713797731.22017.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-4/article_1713797731.7120798.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-4/article_1713797732.198164.pdf\n",
      "\n",
      "Page 3\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s42256-023-00729-y.pdf\n",
      "Enregistré sous: NLP-4/article_1713797740.47173.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-4/article_1713797741.9715729.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-4/article_1713797743.2844522.pdf\n",
      "Téléchargement du PDF: https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf\n",
      "Enregistré sous: NLP-4/article_1713797744.546745.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2202.04824.pdf\n",
      "Enregistré sous: NLP-4/article_1713797744.8064919.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/D19-1221.pdf\n",
      "Enregistré sous: NLP-4/article_1713797745.37149.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.646.pdf\n",
      "Enregistré sous: NLP-4/article_1713797746.514231.pdf\n",
      "Téléchargement du PDF: https://discovery.ucl.ac.uk/id/eprint/10149021/1/bjophthalmol-2022-321141.R1_Proof_hi.pdf\n",
      "Enregistré sous: NLP-4/article_1713797746.89233.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.emnlp-main.572.pdf\n",
      "Enregistré sous: NLP-4/article_1713797748.031857.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.aacl-main.55.pdf\n",
      "Enregistré sous: NLP-4/article_1713797749.2579598.pdf\n",
      "\n",
      "Page 4\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.45.pdf\n",
      "Enregistré sous: NLP-4/article_1713797755.172242.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-4/article_1713797755.707971.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-4/article_1713797756.245746.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-4/article_1713797757.377105.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-4/article_1713797758.530765.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.159.pdf\n",
      "Enregistré sous: NLP-4/article_1713797759.665021.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf\n",
      "Enregistré sous: NLP-4/article_1713797760.215714.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2103.06268.pdf\n",
      "Enregistré sous: NLP-4/article_1713797760.443878.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-4/article_1713797760.938953.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-4/article_1713797761.549911.pdf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c12dc9",
   "metadata": {},
   "source": [
    "# 2 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d1b2b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez saisir votre requête : I'm looking for articles on NLP\n",
      "Langue détectée ==> Anglais\n",
      "Entrez le nombre de pages à parcourir : 4\n",
      "\n",
      "Page 1\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/1902.00751.pdf\n",
      "Enregistré sous: NLP-4/article_989f9a38-e214-4208-933b-381bef175765.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1355.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/P19-1355.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/P19-1355.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2005.11401.pdf\n",
      "Enregistré sous: NLP-4/article_6571d02d-4d56-4eba-9bd1-ccbaa3398bf9.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.340.pdf\n",
      "Enregistré sous: NLP-4/article_d012e96f-22e9-4550-a13e-335bba7aeb51.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.442.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.acl-main.442.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.442.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.168.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2021.naacl-main.168.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2021.naacl-main.168.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.findings-acl.84.pdf\n",
      "Enregistré sous: NLP-4/article_00e61d63-1d18-478b-9131-542761b03365.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1452.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/P19-1452.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/P19-1452.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.acl-main.485.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n",
      "\n",
      "Page 2\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.560.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.acl-main.560.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.560.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.386.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.acl-main.386.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.386.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.insights-1.1.pdf\n",
      "Enregistré sous: NLP-4/article_e98f876f-25be-4fbc-b035-9a1e0c8732e0.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.324.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2021.naacl-main.324.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2021.naacl-main.324.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.500.pdf\n",
      "Enregistré sous: NLP-4/article_10686a54-2be9-4411-a8a7-93b49140755c.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.482.pdf\n",
      "Enregistré sous: NLP-4/article_02c0a545-19f3-43ab-a9b6-71e320b433be.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/N19-4010.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/N19-4010.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "\n",
      "Page 3\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s42256-023-00729-y.pdf\n",
      "Enregistré sous: NLP-4/article_96973213-37ea-4475-ada5-14d524d1f6bc.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-4/article_2139f502-345e-4723-bd2f-5b5e1618fba7.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-4/article_293d1ab6-fbac-4950-b761-c9b4a4c43f5f.pdf\n",
      "Téléchargement du PDF: https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf\n",
      "Enregistré sous: NLP-4/article_f6b21217-ab8c-4500-ad5c-8a9dab030b3a.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2202.04824.pdf\n",
      "Enregistré sous: NLP-4/article_c0be011e-b987-420e-96c0-540c35024827.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/D19-1221.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/D19-1221.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/D19-1221.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.646.pdf\n",
      "Enregistré sous: NLP-4/article_52c6fc1f-be87-416b-b584-df6232ad53c0.pdf\n",
      "Téléchargement du PDF: https://discovery.ucl.ac.uk/id/eprint/10149021/1/bjophthalmol-2022-321141.R1_Proof_hi.pdf\n",
      "Enregistré sous: NLP-4/article_2fab0b93-8c17-4504-afbe-c4adad62b4ae.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.45.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2020.acl-main.45.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2020.acl-main.45.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.emnlp-main.572.pdf\n",
      "Enregistré sous: NLP-4/article_59addde5-e703-46ec-b741-4837f9284874.pdf\n",
      "\n",
      "Page 4\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.aacl-main.55.pdf\n",
      "Enregistré sous: NLP-4/article_2790e258-c763-473a-b833-d92ed3450730.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-4/article_071b32b2-9985-4d37-b70b-79c3914e2ab1.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-4/article_aa7a66a6-edba-4b50-8c2c-16008577461e.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-4/article_8e23e1a7-1a0a-48ab-9a52-650e3d1850a9.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-4/article_0b8e0e00-b8c5-41ee-b675-2a1245844f7f.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.159.pdf\n",
      "Enregistré sous: NLP-4/article_e4d2697b-5b0f-4707-b555-17554b00c40b.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2103.06268.pdf\n",
      "Enregistré sous: NLP-4/article_1794f872-3f88-4cf0-9c76-e98c2e27ad07.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/P18-1079.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Erreur lors du téléchargement du PDF https://www.aclweb.org/anthology/P18-1079.pdf: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/P18-1079.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import uuid  # Importer le module uuid pour la génération de noms de fichiers uniques\n",
    "\n",
    "def extraire_mots_cles_fr(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en français.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    requete_utilisateur = requete.lower()\n",
    "    doc = nlp(requete_utilisateur)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" and token.dep_ not in [\"ROOT\",\"obj\"]) or \\\n",
    "           (token.pos_ == \"ADV\" ) or \\\n",
    "           (token.pos_ == \"PRON\" and token.dep_ != \"dep\") or \\\n",
    "           (token.dep_ == \"ROOT\" and token.pos_ not in [\"NOUN\",\"VERB\"]) or \\\n",
    "           (token.pos_ == \"ADJ\" ) or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ in [\"nmod\",\"punct\"]):\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def extraire_mots_cles_en(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en anglais.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(requete)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" ) or  (token.dep_ == \"punct\") or \\\n",
    "           (token.pos_ == \"ADV\") or (token.dep_ == \"pobj\") or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ != \"dep\") or \\\n",
    "           (token.pos_ == \"ADJ\" ) :\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Fonction pour détecter la langue d'un texte donné.\n",
    "    \n",
    "    Arguments :\n",
    "        - text : Le texte à analyser\n",
    "        \n",
    "    Returns :\n",
    "        - 'français' si la langue détectée est le français, sinon 'anglais'\n",
    "    \"\"\"\n",
    "    lang = detect(text)\n",
    "    if lang == 'fr':\n",
    "        return 'français'\n",
    "    else:\n",
    "        return 'anglais'\n",
    "\n",
    "def format_mot(motcle):\n",
    "    \"\"\"\n",
    "    Fonction pour formater un mot-clé pour une utilisation dans une URL de recherche.\n",
    "    \n",
    "    Arguments :\n",
    "        - motcle : Le mot-clé à formater\n",
    "        \n",
    "    Returns :\n",
    "        - format_motcle : Le mot-clé formaté pour une URL de recherche\n",
    "    \"\"\"\n",
    "    if ' ' in motcle:\n",
    "        format_motcle = motcle.replace(' ', '+')\n",
    "    else:\n",
    "        format_motcle = motcle\n",
    "    return format_motcle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale du programme.\n",
    "    \"\"\"\n",
    "    # Demander à l'utilisateur le sujet de recherche\n",
    "    requete_utilisateur = input('Veuillez saisir votre requête : ')\n",
    "\n",
    "    # Détection de la langue de la requête\n",
    "    langue = detect_language(requete_utilisateur)\n",
    "\n",
    "    # Sélection de la fonction d'extraction des mots-clés en fonction de la langue détectée\n",
    "    if langue == 'français':\n",
    "        mots_cles_format = extraire_mots_cles_fr(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Français\")\n",
    "    else:\n",
    "        mots_cles_format = extraire_mots_cles_en(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Anglais\")\n",
    "\n",
    "    # Formatage des mots-clés pour une utilisation dans une URL de recherche\n",
    "    search_query = format_mot(mots_cles_format)\n",
    "\n",
    "    # Demander à l'utilisateur le nombre de pages à parcourir\n",
    "    num_pages = int(input(\"Entrez le nombre de pages à parcourir : \"))\n",
    "\n",
    "    # Créer le nom du dossier de destination\n",
    "    folder_name = f\"{search_query.replace('+', '-')}-{num_pages}\"\n",
    "\n",
    "    # Créer le dossier s'il n'existe pas\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Initialiser le navigateur Chrome en mode sans tête\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Parcourir les pages demandées\n",
    "    for page_num in range(1, num_pages+1):\n",
    "        print(f\"\\nPage {page_num}\")\n",
    "        # Ouvrir la page de recherche de Semantic Scholar\n",
    "        driver.get(f\"https://www.semanticscholar.org/search?q={search_query}&sort=relevance&page={page_num}\")\n",
    "\n",
    "        # Attendre quelques secondes pour permettre au contenu de charger\n",
    "        time.sleep(20)\n",
    "\n",
    "        # Obtenir le contenu de la page\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Analyser le contenu HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Trouver tous les liens vers les PDF\n",
    "        pdf_links = soup.select('a[href$=\".pdf\"]')\n",
    "\n",
    "        # Télécharger les PDF et les enregistrer dans le dossier \"articles\"\n",
    "        for link in pdf_links:\n",
    "            pdf_url = link[\"href\"]\n",
    "            print(\"Téléchargement du PDF:\", pdf_url)\n",
    "            try:\n",
    "                # Télécharger le PDF depuis le lien\n",
    "                response = requests.get(pdf_url, stream=True)\n",
    "                response.raise_for_status()  # Vérifier si la requête a réussi\n",
    "                # Générer un nom de fichier unique pour chaque PDF\n",
    "                file_name = f\"article_{uuid.uuid4()}.pdf\"  # Utiliser uuid.uuid4() pour générer un nom de fichier unique\n",
    "                # Enregistrer le PDF dans le dossier de destination\n",
    "                with open(os.path.join(folder_name, file_name), \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Enregistré sous: {folder_name}/{file_name}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"Une erreur inattendue s'est produite : {str(ex)}\")\n",
    "\n",
    "    # Fermer le navigateur\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b390439",
   "metadata": {},
   "source": [
    "# 3 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e74db104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez saisir votre requête : I'm looking for articles on NLP\n",
      "Langue détectée ==> Anglais\n",
      "Entrez le nombre de pages à parcourir : 5\n",
      "\n",
      "Page 1\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/1902.00751.pdf\n",
      "Enregistré sous: NLP-5/article_b8cdcf69-a2d1-43a9-9a87-b0c4200d64f3.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1355.pdf\n",
      "Enregistré sous: NLP-5/article_4c69a0fc-0fad-41fe-bfdd-68d19ac66ebc.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2005.11401.pdf\n",
      "Enregistré sous: NLP-5/article_51c6bfbc-e60d-474c-ae96-2275590f44ee.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.340.pdf\n",
      "Enregistré sous: NLP-5/article_10e0a06f-06a2-434e-b0d7-9e88bc9e4e47.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.442.pdf\n",
      "Enregistré sous: NLP-5/article_3700a370-835e-4d28-b6fd-c0bed26661c3.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.168.pdf\n",
      "Enregistré sous: NLP-5/article_293f073b-0f29-45e5-b3d1-34da0b284251.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.findings-acl.84.pdf\n",
      "Enregistré sous: NLP-5/article_0fcef0b1-fa65-4c3e-ae2d-21849238395b.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1452.pdf\n",
      "Enregistré sous: NLP-5/article_68e53962-2e74-4b22-8abf-851fefec0550.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n",
      "Enregistré sous: NLP-5/article_5474a016-f76e-4d6d-a2e8-323416d49e0b.pdf\n",
      "\n",
      "Page 2\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-5/article_c0d41391-ef9e-4d94-974d-8b78db3e3311.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-5/article_56028ac2-2cad-414e-aede-43e953080845.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.560.pdf\n",
      "Enregistré sous: NLP-5/article_4e0c792d-f834-46b9-b481-bb39668a9a10.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.386.pdf\n",
      "Enregistré sous: NLP-5/article_e2bd876e-db62-4162-843c-730a58bfc4bb.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.insights-1.1.pdf\n",
      "Enregistré sous: NLP-5/article_bccb22bb-066c-465c-a534-7f4df3e5bf43.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.324.pdf\n",
      "Enregistré sous: NLP-5/article_820d7629-d753-4b21-a44d-223e7878b624.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.500.pdf\n",
      "Enregistré sous: NLP-5/article_8600a7c1-15a0-45a8-978f-6418597c49a4.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.482.pdf\n",
      "Enregistré sous: NLP-5/article_3e431e75-7d5a-4caa-bd37-37171237350b.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-5/article_2eee09bb-2b58-4ad0-8183-5349bfcdb788.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-5/article_1a670258-380b-4b2a-9739-8496f3388351.pdf\n",
      "\n",
      "Page 3\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s42256-023-00729-y.pdf\n",
      "Enregistré sous: NLP-5/article_bd47d835-6252-4998-a087-42422f9862b6.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-5/article_e0416cd9-7d2f-4f88-bd13-a0173dfd15f0.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-5/article_57b51c6a-3ea1-4c31-9dd6-0378d25a9ead.pdf\n",
      "Téléchargement du PDF: https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf\n",
      "Enregistré sous: NLP-5/article_9787cb59-c11b-4332-b596-bfb8c311347f.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2202.04824.pdf\n",
      "Enregistré sous: NLP-5/article_0bd38a6f-fb96-400f-932e-92b1871054be.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/D19-1221.pdf\n",
      "Enregistré sous: NLP-5/article_7669c859-d427-4aad-801b-244f4000da31.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.646.pdf\n",
      "Enregistré sous: NLP-5/article_e6a94623-9fdb-4467-94be-edfa3bd0654e.pdf\n",
      "Téléchargement du PDF: https://discovery.ucl.ac.uk/id/eprint/10149021/1/bjophthalmol-2022-321141.R1_Proof_hi.pdf\n",
      "Enregistré sous: NLP-5/article_00ad38e1-9359-45a9-8740-aba2315badf5.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.45.pdf\n",
      "Enregistré sous: NLP-5/article_901ac633-b737-48f2-99a6-9f6c53e236c8.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.emnlp-main.572.pdf\n",
      "Enregistré sous: NLP-5/article_a113d90d-41fe-4efe-8638-3cebcba14a64.pdf\n",
      "\n",
      "Page 4\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.aacl-main.55.pdf\n",
      "Enregistré sous: NLP-5/article_e1dd4597-3bf3-46c2-9e6e-11fe2dce15ba.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-5/article_387d182c-2b81-4a2a-9944-93d668e16da6.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-5/article_dff095cd-2202-4cf7-b57a-aa56d0e41110.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-5/article_eb724cb8-43e1-49a4-b443-049c18942ab2.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-5/article_69d1ce99-4f13-4094-b163-312e0a73821d.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.159.pdf\n",
      "Enregistré sous: NLP-5/article_dd96ec56-a32e-4093-b892-42b27ef2fcd6.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf\n",
      "Enregistré sous: NLP-5/article_3a9786c9-86cc-4fbf-8bb3-d180accc59b7.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2103.06268.pdf\n",
      "Enregistré sous: NLP-5/article_f86ea021-a3e8-4c5f-be33-badedac6e082.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-5/article_61bac5f9-5044-4765-8af6-770d369f5e10.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-5/article_1844ede4-d444-4b1f-868e-41247b1df107.pdf\n",
      "\n",
      "Page 5\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.13.pdf\n",
      "Enregistré sous: NLP-5/article_11afce49-9fe0-4097-b0c6-46a8f8e35815.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.13.pdf\n",
      "Enregistré sous: NLP-5/article_e26098a1-438f-4387-903b-c13bd83589e7.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.165.pdf\n",
      "Enregistré sous: NLP-5/article_81b89447-ad99-4ba0-941e-7dd0413e7b8a.pdf\n",
      "Téléchargement du PDF: https://pdfs.semanticscholar.org/c698/6906d5b0ca40f3f2ecc45646bf0fb9587a3e.pdf\n",
      "Enregistré sous: NLP-5/article_a0770644-5fe1-4b22-b590-a01f68d05078.pdf\n",
      "Téléchargement du PDF: https://research.fb.com/wp-content/uploads/2021/09/Transformers4Rec-Bridging-the-Gap-between-NLP-and-Sequential-Session-Based-Recommendation-1.pdf\n",
      "Enregistré sous: NLP-5/article_3c0831bc-f851-4106-a9eb-90481b6b380a.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.tacl-1.12.pdf\n",
      "Enregistré sous: NLP-5/article_c945aba3-7ef2-4a1b-8e12-9a1c8815d612.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.339.pdf\n",
      "Enregistré sous: NLP-5/article_429579ec-6861-4d94-9505-81b70ab55160.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.487.pdf\n",
      "Enregistré sous: NLP-5/article_aa706608-c061-4111-94e5-4c5ab6c48c33.pdf\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s41467-020-19266-y.pdf\n",
      "Enregistré sous: NLP-5/article_f817afd0-cc2b-4813-8b89-107f7b45463f.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.coling-main.66.pdf\n",
      "Enregistré sous: NLP-5/article_261cb9f2-9f37-4d0b-8eab-18eac175daab.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import uuid  # Importer le module uuid pour la génération de noms de fichiers uniques\n",
    "\n",
    "def extraire_mots_cles_fr(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en français.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    requete_utilisateur = requete.lower()\n",
    "    doc = nlp(requete_utilisateur)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" and token.dep_ not in [\"ROOT\",\"obj\"]) or \\\n",
    "           (token.pos_ == \"ADV\" ) or \\\n",
    "           (token.pos_ == \"PRON\" and token.dep_ != \"dep\") or \\\n",
    "           (token.dep_ == \"ROOT\" and token.pos_ not in [\"NOUN\",\"VERB\"]) or \\\n",
    "           (token.pos_ == \"ADJ\" ) or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ in [\"nmod\",\"punct\"]):\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def extraire_mots_cles_en(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en anglais.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(requete)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" ) or  (token.dep_ == \"punct\") or \\\n",
    "           (token.pos_ == \"ADV\") or (token.dep_ == \"pobj\") or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ != \"dep\") or \\\n",
    "           (token.pos_ == \"ADJ\" ) :\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Fonction pour détecter la langue d'un texte donné.\n",
    "    \n",
    "    Arguments :\n",
    "        - text : Le texte à analyser\n",
    "        \n",
    "    Returns :\n",
    "        - 'français' si la langue détectée est le français, sinon 'anglais'\n",
    "    \"\"\"\n",
    "    lang = detect(text)\n",
    "    if lang == 'fr':\n",
    "        return 'français'\n",
    "    else:\n",
    "        return 'anglais'\n",
    "\n",
    "def format_mot(motcle):\n",
    "    \"\"\"\n",
    "    Fonction pour formater un mot-clé pour une utilisation dans une URL de recherche.\n",
    "    \n",
    "    Arguments :\n",
    "        - motcle : Le mot-clé à formater\n",
    "        \n",
    "    Returns :\n",
    "        - format_motcle : Le mot-clé formaté pour une URL de recherche\n",
    "    \"\"\"\n",
    "    if ' ' in motcle:\n",
    "        format_motcle = motcle.replace(' ', '+')\n",
    "    else:\n",
    "        format_motcle = motcle\n",
    "    return format_motcle\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale du programme.\n",
    "    \"\"\"\n",
    "    # Demander à l'utilisateur le sujet de recherche\n",
    "    requete_utilisateur = input('Veuillez saisir votre requête : ')\n",
    "\n",
    "    # Détection de la langue de la requête\n",
    "    langue = detect_language(requete_utilisateur)\n",
    "\n",
    "    # Sélection de la fonction d'extraction des mots-clés en fonction de la langue détectée\n",
    "    if langue == 'français':\n",
    "        mots_cles_format = extraire_mots_cles_fr(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Français\")\n",
    "    else:\n",
    "        mots_cles_format = extraire_mots_cles_en(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Anglais\")\n",
    "\n",
    "    # Formatage des mots-clés pour une utilisation dans une URL de recherche\n",
    "    search_query = format_mot(mots_cles_format)\n",
    "\n",
    "    # Demander à l'utilisateur le nombre de pages à parcourir\n",
    "    num_pages = int(input(\"Entrez le nombre de pages à parcourir : \"))\n",
    "\n",
    "    # Créer le nom du dossier de destination\n",
    "    folder_name = f\"{search_query.replace('+', '-')}-{num_pages}\"\n",
    "\n",
    "    # Créer le dossier s'il n'existe pas\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Initialiser le navigateur Chrome en mode sans tête\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Parcourir les pages demandées\n",
    "    for page_num in range(1, num_pages+1):\n",
    "        print(f\"\\nPage {page_num}\")\n",
    "        # Ouvrir la page de recherche de Semantic Scholar\n",
    "        driver.get(f\"https://www.semanticscholar.org/search?q={search_query}&sort=relevance&page={page_num}\")\n",
    "\n",
    "        # Attendre quelques secondes pour permettre au contenu de charger\n",
    "        time.sleep(20)\n",
    "\n",
    "        # Obtenir le contenu de la page\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Analyser le contenu HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Trouver tous les liens vers les PDF\n",
    "        pdf_links = soup.select('a[href$=\".pdf\"]')\n",
    "\n",
    "        # Télécharger les PDF et les enregistrer dans le dossier \"articles\"\n",
    "        for link in pdf_links:\n",
    "            pdf_url = link[\"href\"]\n",
    "            print(\"Téléchargement du PDF:\", pdf_url)\n",
    "            try:\n",
    "                # Ajouter un en-tête utilisateur pour simuler une demande de navigateur standard\n",
    "                headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.1 Safari/537.36\"}\n",
    "                # Télécharger le PDF depuis le lien en incluant les en-têtes\n",
    "                response = requests.get(pdf_url, headers=headers, stream=True)\n",
    "                response.raise_for_status()  # Vérifier si la requête a réussi\n",
    "                # Générer un nom de fichier unique pour chaque PDF\n",
    "                file_name = f\"article_{uuid.uuid4()}.pdf\"  # Utiliser uuid.uuid4() pour générer un nom de fichier unique\n",
    "                # Enregistrer le PDF dans le dossier de destination\n",
    "                with open(os.path.join(folder_name, file_name), \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Enregistré sous: {folder_name}/{file_name}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"Une erreur inattendue s'est produite : {str(ex)}\")\n",
    "\n",
    "    # Fermer le navigateur\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6970e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte extrait du PDF :\n",
      " Transformers4Rec: Bridging the Gap between NLP and\n",
      "Sequential / Session-Based Recommendation\n",
      "Gabriel de Souza Pereira\n",
      "Moreira\n",
      "gmoreira@nvidia.com\n",
      "NVIDIA\n",
      "São Paulo, BrazilSara Rabhi\n",
      "srabhi@nvidia.com\n",
      "NVIDIA\n",
      "Ontario, CanadaJeong Min Lee∗\n",
      "jeongmin@fb.com\n",
      "Facebook AI\n",
      "California, United States\n",
      "Ronay Ak\n",
      "ronaya@nvidia.com\n",
      "NVIDIA\n",
      "Florida, United StatesEven Oldridge\n",
      "eoldridge@nvidia.com\n",
      "NVIDIA\n",
      "British Columbia, Canada\n",
      "ABSTRACT\n",
      "Much of the recent progress in sequential and session-based rec-\n",
      "ommendation has been driven by improvements in model archi-\n",
      "tecture and pretraining techniques originating in the field of Natu-\n",
      "ral Language Processing. Transformer architectures in particular\n",
      "have facilitated building higher-capacity models and provided data\n",
      "augmentation and training techniques which demonstrably im-\n",
      "prove the effectiveness of sequential recommendation. But with a\n",
      "thousandfold more research going on in NLP, the application of\n",
      "transformers for recommendation understandably lags behind. To\n",
      "remedy this we introduce Transformers4Rec, an open-source li-\n",
      "brary built upon HuggingFace’s Transformers library with a similar\n",
      "goal of opening up the advances of NLP based Transformers to the\n",
      "recommender system community and making these advancements\n",
      "immediately accessible for the tasks of sequential and session-based\n",
      "recommendation. Like its core dependency, Transformers4Rec is\n",
      "designed to be extensible by researchers, simple for practitioners,\n",
      "and fast and robust in industrial deployments.\n",
      "In order to demonstrate the usefulness of the library and the ap-\n",
      "plicability of Transformer architectures in next-click prediction for\n",
      "user sessions, where sequence lengths are much shorter than those\n",
      "commonly found in NLP, we have leveraged Transformers4Rec\n",
      "to win two recent session-based recommendation competitions.\n",
      "In addition, we present in this paper the first comprehensive em-\n",
      "pirical analysis comparing many Transformer architectures and\n",
      "training approaches for the task of session-based recommendation.\n",
      "We demonstrate that the best Transformer architectures have supe-\n",
      "rior performance across two e-commerce datasets while performing\n",
      "similarly to the baselines on two news datasets. We further evaluate\n",
      "in isolation the effectiveness of the different training techniques\n",
      "∗Most of the contribution with this project was done during his internship at NVIDIA.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "©2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-8458-2/21/09. . . $15.00\n",
      "https://doi.org/10.1145/3460231.3474255used in causal language modeling, masked language modeling, per-\n",
      "mutation language modeling and replacement token detection for a\n",
      "single Transformer architecture, XLNet. We establish that training\n",
      "XLNet with replacement token detection performs well across all\n",
      "datasets. Finally, we explore techniques to include side information\n",
      "such as item and user context features in order to establish best prac-\n",
      "tices and show that the inclusion of side information uniformly im-\n",
      "proves recommendation performance. Transformers4Rec library is\n",
      "available at https://github.com/NVIDIA-Merlin/Transformers4Rec/\n",
      "ACM Reference Format:\n",
      "Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak,\n",
      "and Even Oldridge. 2021. Transformers4Rec: Bridging the Gap between\n",
      "NLP and Sequential / Session-Based Recommendation. In Fifteenth ACM\n",
      "Conference on Recommender Systems (RecSys ’21), September 27-October 1,\n",
      "2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 11 pages. https:\n",
      "//doi.org/10.1145/3460231.3474255\n",
      "1 INTRODUCTION\n",
      "Recommender systems improve users experience in online services\n",
      "like e-commerce, news portals, social networks, media platforms,\n",
      "and many others, by providing personalized suggestions and help-\n",
      "ing users deal with large catalogs of items and information overload.\n",
      "In recent years we have observed an increased research interest\n",
      "in sequential recommendation approaches [ 11] which explicitly\n",
      "model sequences of user interactions to better infer preference\n",
      "or context changes over time. In a number of these settings only\n",
      "the most recent interactions are available, and in all domains for\n",
      "fresh or anonymous users only the interactions from the current\n",
      "user session are available. This common scenario is addressed by a\n",
      "sub-genre of sequential recommendation known as session-based\n",
      "recommendation [ 35,56], in which only a short sequence of very re-\n",
      "cent user interactions is available for predicting the next in-session\n",
      "interaction.\n",
      "Over the past decade there has been a trend toward leveraging\n",
      "and adapting approaches proposed by Natural Language Processing\n",
      "(NLP) research like Word2Vec [ 37], GRU [ 7], and Attention [ 3]\n",
      "for recommender systems (RecSys). The phenomena is especially\n",
      "noticeable for sequential and session-based recommendation where\n",
      "the sequential processing of users interactions is analogous to the\n",
      "language modeling (LM) task and many key RecSys architectures\n",
      "have been adapted from NLP, like GRU4Rec [ 18] – the seminalRecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Recurrent Neural Network (RNN)-based architecture for session-\n",
      "based recommendation.\n",
      "More recently, Transformer architectures [ 55] have become the\n",
      "dominant technique over convolutional and recurrent neural net-\n",
      "works for language modeling tasks. Because of their efficient paral-\n",
      "lel training, these architectures scale well with training data and\n",
      "model size, and are effective at modeling long-range sequences.\n",
      "They have similarly been applied to sequential recommendation in\n",
      "architectures like SASRec [ 24], BERT4Rec [ 50] and BST [ 5] and to\n",
      "session-based recommendation in [6, 12, 33, 44, 51, 62, 66].\n",
      "The HuggingFace (HF) Transformers library [ 57] was \"estab-\n",
      "lished with the goal of opening up advancements in NLP to the wider\n",
      "machine learning community\" and has become very popular among\n",
      "NLP researchers and practitioners, providing standardized imple-\n",
      "mentations of the state-of-the-art Transformer architectures pro-\n",
      "duced by the research community, often within days or weeks of\n",
      "their publication. In this paper we introduce Transformers4Rec1,\n",
      "an open-source library which adapts and extends the HF Trans-\n",
      "formers library for use in recommender systems. With Transform-\n",
      "ers4Rec, RecSys researchers and practitioners can easily experiment\n",
      "with the latest NLP Transformer architectures for sequential and\n",
      "session-based recommendation tasks and deploy those models into\n",
      "production.\n",
      "We have leveraged and evolved the Transformers4Rec library to\n",
      "win two recent session-based recommendation competitions: the\n",
      "WSDM WebTour Workshop Challenge 2021, organized by Book-\n",
      "ing.com [ 48], and the SIGIR eCommerce Workshop Data Challenge\n",
      "2021, organized by Coveo [42].\n",
      "In this paper, we used the Transformers4Rec library to conduct\n",
      "a comprehensive empirical analysis of session-based recommen-\n",
      "dation, comparing different Transformer architectures and train-\n",
      "ing approaches with popular session-based recommendation base-\n",
      "lines. Our experiments which we discuss in detail in Section 4 on\n",
      "two e-commerce and two news portals datasets show that modern\n",
      "Transformers architectures improve the accuracy of session-based\n",
      "recommendation compared to the best neural and non-neural base-\n",
      "lines. In addition we also examine the effect of applying different\n",
      "training techniques – Causal LM (CLM), Masked LM (MLM), Per-\n",
      "mutation LM (PLM), and Replacement Token Detection (RTD) for\n",
      "a single Transformer architecture - XLNet[ 63]. Finally, we explore\n",
      "several techniques to include side information such as item and\n",
      "user context features in order to establish best practices and im-\n",
      "prove recommendation performance even further. Specifically in\n",
      "this work we investigate the following research questions:\n",
      "RQ1: Can transformer-based architectures provide accurate\n",
      "next-click predictions for the usually short user sequences\n",
      "found in the session-based recommendation task?\n",
      "RQ2: How do the training techniques of CLM, MLM, PLM,\n",
      "and RTD comparatively perform for the task of session-based\n",
      "recommendation?\n",
      "RQ3: What are effective approaches to integrate additional\n",
      "features, commonly referred to as side information, into\n",
      "transformer architectures in order to improve recommenda-\n",
      "tion performance?\n",
      "Our contributions in this paper are threefold:\n",
      "1https://github.com/NVIDIA-Merlin/Transformers4Rec/(1)We provide an overview of the relationship between the\n",
      "fields of NLP and Sequential / Session-Based recommenda-\n",
      "tion, highlighting similarities and differences between the\n",
      "algorithms developed in each field.\n",
      "(2)We introduce the open-source library Transformers4Rec\n",
      "which wraps the widely popular HuggingFace’s Transform-\n",
      "ers library and allows researchers and practitioners in the\n",
      "RecSys community to quickly and easily explore transformer\n",
      "architectures in the context of sequential and session-based\n",
      "recommendation.\n",
      "(3)We perform an empirical analysis with broad experimen-\n",
      "tation of modern NLP based Transformer architectures for\n",
      "the task of session-based recommendation, establishing their\n",
      "performance relative to popular baselines. We further com-\n",
      "pare CLM, MLM, PLM, and RTD as training techniques for\n",
      "session-based recommendation using XLNet. Finally we ex-\n",
      "amine three methods of adding side information to session-\n",
      "based recommenders.\n",
      "Our motivation in the development of Transformers4Rec is to\n",
      "enable researchers and practitioners alike leverage the latest devel-\n",
      "opments of NLP within the context of sequential and session-based\n",
      "recommendation and to take advantage of the vast amount of re-\n",
      "search and development happening there, bridging the gap between\n",
      "these two communities.\n",
      "2 THE RELATIONSHIP BETWEEN NLP AND\n",
      "RECSYS RESEARCH\n",
      "The field of NLP has evolved significantly over the past eight years,\n",
      "particularly due to the increased usage of deep learning. Mirroring\n",
      "this, state of the art NLP approaches have inspired RecSys prac-\n",
      "titioners and researchers to adapt those architectures, especially\n",
      "for sequential and session-based recommendation problems, as\n",
      "illustrated in Figure 1.\n",
      "Early neural language models [ 36,45] focused on learning rep-\n",
      "resentations where words with similar syntax and meaning are\n",
      "represented in the same regions of the vector space. The distributed\n",
      "vector representations methods were then extended to represent\n",
      "sentences and paragraphs [ 25]. In the RecSys domain these neural\n",
      "methods were adapted to learn item, user or context embeddings by\n",
      "taking their co-occurrence into account within the users’ history\n",
      "of interactions. Prod2Vec [ 14] learnt product representations using\n",
      "the Word2Vec skip-gram model [ 36]. Similar to Doc2Vec [ 25], Meta-\n",
      "Prod2Vec [ 54] extended Prod2Vec objectives by including items’\n",
      "metadata in the neural network. The pre-trained embeddings were\n",
      "then used to recommend the most similar products to the query\n",
      "item.\n",
      "Another popular NLP-inspired approach for Recommender Sys-\n",
      "tems was the usage of RNNs for sequential and session-based rec-\n",
      "ommendation, using the sequential nature of item interactions in a\n",
      "users’ session and their past visits. GRU4REC [ 18] applied a GRU\n",
      "model to session-based recommendation, generating next-click pre-\n",
      "diction for an active session. Unlike the relatively small fixed-size\n",
      "word vocabulary in NLP, the set of items in recommender systems\n",
      "is often very large and fast training of a scalable model is important.\n",
      "To overcome this GRU4REC included different ranking pairwise\n",
      "loss functions which allow for more efficient training, a techniqueTransformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Figure 1: A timeline illustrating the influence of NLP research in Recommender Systems\n",
      "that we also support in Transformers4Rec. Following GRU4Rec,\n",
      "some works explored techniques to include additional features (side\n",
      "information) other than item ids [ 20,40]. RNN-based model were\n",
      "improved in [ 53] by employing data augmentation and a method\n",
      "to account for shifts in the input data distribution.\n",
      "In 2016 the architectures that dominated NLP began to change\n",
      "when attention mechanisms [ 3] were introduced, demonstrating\n",
      "their effectiveness on long sequences. Attention was originally\n",
      "applied to recommender systems in the Neural Attentive Recom-\n",
      "mendation Machine (NARM) architecture [ 26] which incorporated\n",
      "an attention mechanism into an RNN architecture as an additional\n",
      "layer, proposing that attention was able to capture not only the\n",
      "user’s sequential behavior but also their main purpose in the cur-\n",
      "rent session. Conversely, Attentional FM [ 61] used an attention\n",
      "network to learn the importance of feature interactions in the non\n",
      "sequential Factorized Machines model.\n",
      "2.1 Transformers for NLP\n",
      "In 2017 the seminal Transformer architecture [ 55] was introduced\n",
      "as an efficent alternative to the RNN-based sequential encoder-\n",
      "decoder network with self-attention. The self-attention mechanism\n",
      "is capable of representing dependencies within the sequence of to-\n",
      "kens, favors parallel processing and scales nicely for long sequences.\n",
      "With its host of benefits, transformer architectures became the de-\n",
      "fault choice for the majority of NLP tasks, and many variants such\n",
      "as GPT-2 [ 47], BERT [ 10], XLNet [ 63] followed. All these models\n",
      "proposed novel pre-training approaches for language modelling\n",
      "and adapted the fundamental building block of self-attention to\n",
      "take into account language modeling specificities.\n",
      "GPT-2 [ 47] pre-trained a stack of Transformer decoder blocks\n",
      "using a Causal LM approach where the next token is predicted\n",
      "given the context of the previous ones. To avoid information leak-\n",
      "age from the right context, it was proposed a masked self-attention\n",
      "mechanism where each token has only access to its previous tokens\n",
      "hidden states. BERT [ 10] used the Transformer encoder block in-\n",
      "stead and represented the input as a sum of the word’s embeddings\n",
      "and its absolute position embeddings. Unlike GPT2, BERT used\n",
      "the fully contextual self-attention mechanism where the word has\n",
      "simultaneously access to past and future contexts. To leverage infor-\n",
      "mation from both directions, BERT introduced Masked LM training\n",
      "which randomly masks 15% of the tokens in the input sequence,\n",
      "and requires the encoder model to predict the original tokens using\n",
      "non-masked information from its surroundings. However, the inputof GPT2 and BERT is a fixed-size block of words requiring segment-\n",
      "ing long texts into chunks without respecting the natural structure\n",
      "of sentences or paragraphs. Transformer-XL [ 9] solved the context\n",
      "fragmentation by introducing a segment recurrence mechanism\n",
      "where the hidden states of previous blocks are cached and used\n",
      "to extend the context of the new upcoming segment. Moreover\n",
      "Transformer-XL replaced the absolute position embeddings input\n",
      "by a relative positional encoding vector directly incorporated in the\n",
      "self-attention layer. Similar to GPT-2, the model was pre-trained\n",
      "using Causal LM and a masked self-attention mechanism.\n",
      "In addition to the context fragmentation issue, masking the in-\n",
      "put in BERT results produces a discrepancy between pre-training\n",
      "and fine-tuning. XLNet [ 63] defined yet another pre-training ap-\n",
      "proach, called Permutation LM, which keeps the original input\n",
      "sequence and uses a permutation factorization at the level of the\n",
      "self-attention layer to define the accessible bidirectional context.\n",
      "At each iteration, the context of the target token is defined based\n",
      "on its position index in the permutation. As the target token is not\n",
      "masked, XLNet extends Transformer-XL self-attention mechanism\n",
      "with an additional stream, called query stream attention, which\n",
      "represents the relative position without accessing content.\n",
      "BERT and XLNet pre-training tasks were defined over a small\n",
      "subset of tokens in the sequence. To learn from all input tokens,\n",
      "ELECTRA [ 8] added a discriminator network to BERT that is pre-\n",
      "trained to predict for every token whether it is an original or an\n",
      "artificial replacement. The Replacement Token Detection (RTD) task\n",
      "consists of replacing masked positions of the Masked LM task by\n",
      "random tokens. The general generator-discriminator architecture\n",
      "was jointly pre-trained using Masked LM and RTD tasks. In our\n",
      "empirical analysis we experiment with many of the Transformer\n",
      "architectures and training approaches described in this section for\n",
      "the session-based recommendation task.\n",
      "2.2 Transformers for Sequential\n",
      "Recommendation\n",
      "The empirical study conducted in RoBERTa [ 29] demonstrated the\n",
      "effectiveness of Transformer-based architectures when trained on\n",
      "long sequences and over large set of pre-training data, motivat-\n",
      "ing RecSys researchers to use and adapt these architectures for\n",
      "sequential recommendation. AttRec was proposed in [ 64], utilizing\n",
      "the self-attention mechanism to infer the item-item relationship\n",
      "from the user’s historical interactions and estimate weights of each\n",
      "item in the user’s trajectories. They also proposed a collaborative\n",
      "metric learning component to model user long-term preference.RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Analogous to GPT-2, SASRec [ 24] used CLM for training, predict-\n",
      "ing the next item in the sequence from only past user interactions.\n",
      "BERT4Rec [ 50] was a follow-up work, improving upon SASRec by\n",
      "demonstrating improved accuracy training the network with the\n",
      "MLM approach. While BERT used MLM as a pre-training phase\n",
      "to learn words representation vectors and then fine-tuned the pre-\n",
      "trained model for downstream tasks evaluation, BERT4Rec used\n",
      "MLM as an end-to-end task for training and evaluation. Because\n",
      "this approach leaks future information on training, they ensured\n",
      "that during inference only the last item of the sequence is masked,\n",
      "making it compatible with the next-click prediction task. SSE-PT\n",
      "[59] is similar to SASRec, but proposes a personalization approach\n",
      "by concatenating a user embedding to the item embedding, to rep-\n",
      "resent an interaction. To avoid overfitting with the addition of\n",
      "the user embedding they propose the usage of Stochastic Shared\n",
      "Embeddings (SSE), also known as swap noise , in which user em-\n",
      "beddings are stochastically replaced by other embeddings with a\n",
      "probability. The experiments of AttRec, SASRec, BERT4Rec and\n",
      "SSE-PT experiments were performed on longer user sequences of\n",
      "interactions, with average sequence length by dataset between 8.8\n",
      "and 1655 and global average of 73.\n",
      "As sequences of user interactions can span many months of\n",
      "data and users preferences might change over time, the elapsed\n",
      "time between user interactions are important for predicting cur-\n",
      "rent interests. Unlike the ordered sequence of words, positional\n",
      "embeddings of user interactions should account for the irregular\n",
      "time span between consecutive items. In [ 67], they found it very\n",
      "difficult learning a good embedding directly on this continuous\n",
      "time feature using embedding concatenation or addition. Instead,\n",
      "they discretize the elapsed time between interactions at the log\n",
      "scale, and represent it as a categorical feature embedding.\n",
      "Besides time span feature, the use of side information in indus-\n",
      "try settings is common and has been explored, especially by the\n",
      "Alibaba Group [ 5,34,67]. ATRank [ 67] models heterogeneous user\n",
      "behaviors with multiple latent semantic spaces. It splits features\n",
      "by behaviour groups (e.g. item, search), which are concatenated\n",
      "and processed via self-attention. Behaviour Sequence Transformer\n",
      "(BST) [ 5] employs Transformers for the Click-Through Rate (CTR)\n",
      "prediction task, where embedding vectors of item id and category id\n",
      "are concatenated and fed into Transformer and later concatenated\n",
      "with other user contextual features. Sequential Deep Matching\n",
      "(SDM) [ 34] concatenates categorical input features like categories,\n",
      "brands and shops and feeds to a self-attention module. We include\n",
      "side information modelling in our work as a part of RQ3 where we\n",
      "investigate effective approaches to represent continuous numerical\n",
      "features and to combine them with categorical features.\n",
      "Finally, the Contextual Self-Attention Network (CSAN) [ 21] ex-\n",
      "plores user heterogeneous sequential behaviors, which include a\n",
      "diversity of actions and multi-modal content (e.g. structured data,\n",
      "image, text). They propose a feature-wise self-attention to extract\n",
      "different aspects of the sequence to model the complicated correla-\n",
      "tions.2.3 Transformers for Session-based\n",
      "Recommendation\n",
      "Transformers have been shown to outperform RNNs sequential rec-\n",
      "ommendation tasks even in cases where user sessions are shorter\n",
      "than sequences used in NLP, and a number of works have ap-\n",
      "plied Transformers and the Self-Attention mechanism to the task\n",
      "of session-based recommendation [ 6,12,33,44,51,62,66]. The\n",
      "authors of [ 66] argue that some items in a session might be irrele-\n",
      "vant to users preferences or become disturbances for modelling. So\n",
      "they propose a preference-aware mask based on self-attention for\n",
      "better capturing the users preferences over items within sessions.\n",
      "Similarly [ 44] proposed a modified self-attention mechanism to\n",
      "estimate the items importance for a session. In [ 62], self-attention\n",
      "was combined with graph neural networks to capture both short\n",
      "and long-range dependencies and enhance session representations.\n",
      "Most of aforementioned works on session-based recommenda-\n",
      "tion with Transformers use an auto-regressive (CLM) approach,\n",
      "using only interactions before the target item as input. Only [ 6]\n",
      "use a training scheme similar to BERT [ 10] autoencoding (MLM),\n",
      "which allows the usage of future in-session interactions (privileged\n",
      "information) during training.\n",
      "In our work we perform a comprehensive analysis on a wide\n",
      "range of Transformer architectures and training approaches. Our\n",
      "work also uniquely explores different techniques to leverage side\n",
      "information in order to improve recommendation accuracy. Finally,\n",
      "while the other works focus on the e-commerce domain, ours is\n",
      "the first work on Transformers for news recommendation, which\n",
      "poses some specific challenges like shorter sessions, intense user\n",
      "and item cold-start problem and rapid decay of item relevance.\n",
      "3 TRANSFORMERS4REC\n",
      "To effectively bridge the gap between language modeling (NLP)\n",
      "and sequential / session-based recommendation tasks, we have\n",
      "developed Transformers4Rec based upon HuggingFace (HF) Trans-\n",
      "formers library. HF Transformers is an open-source library [ 57]\n",
      "with over 400 contributors that provides standardized efficient im-\n",
      "plementations of recent Transformer architectures, currently 63 and\n",
      "counting. The library is designed for both research and production.\n",
      "Models are composed of three building blocks: (a) a tokenizer, which\n",
      "converts raw text to sparse index encodings; (b) a transformer archi-\n",
      "tecture; and (c) a head for NLP tasks, like Text Classification, Gen-\n",
      "eration, Sentiment Analysis, Translation, Summarization, among\n",
      "others. In our work, we leverage only the transformer architectures\n",
      "building block (b) and their configuration classes, adding specialized\n",
      "heads for the recommendation problem.\n",
      "Sequential and session-based recommendation have some key\n",
      "differences from language models that are specifically addressed by\n",
      "our library: (a) the inclusion of additional side information; (b) the\n",
      "usage of ranking metrics for evaluation; and (c) the emulation of\n",
      "the intense concept drift faced in RecSys compared to NLP models\n",
      "through the use of temporal incremental training and evaluation,\n",
      "as will be described in more detail later. With Transformers4Rec,\n",
      "the RecSys community can immediately make use of state-of-the-\n",
      "art NLP research and leverage novel Transformer architectures,\n",
      "exploring them for different RecSys use cases and datasets, and\n",
      "identifying which ones perform better.Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Figure 2: Transformers4Rec pipeline overview\n",
      "3.1 Transformers4Rec Pipeline\n",
      "As shown in Figure 2, Transformers4Rec is an end-to-end RecSys\n",
      "framework that encompasses data pre-processing, model training\n",
      "and evaluation. The framework was developed in Python, with\n",
      "PyTorch2and HF Transformers as its core dependencies, building\n",
      "upon them to provide a pipeline described next.\n",
      "3.1.1 Data preprocessing and feature engineering. Data pre-processing\n",
      "is a common bottleneck of RecSys production pipelines. It is the fo-\n",
      "cus of NVIDIA NVTabular3library which provides GPU-accelerated\n",
      "preprocessing of terabyte sized recsys datasets. Transformers4Rec\n",
      "and NVTabular are seamless integrated for being co-developed.\n",
      "NVTabular not only supports common and advanced feature en-\n",
      "gineering techniques, but also specialized ops for sequential and\n",
      "session-based recommendation, like grouping time-sorted interac-\n",
      "tions by user or session and truncating the sequences to the first\n",
      "or last N interactions. The preprocessed data is saved to the struc-\n",
      "tured and queryable Parquet format. Transformers4Rec leverages\n",
      "the NVTabular data loader which reads Parquet files directly into\n",
      "GPU memory, making model training and evaluation faster. Trans-\n",
      "formers4Rec also uses as input a configuration file to set which\n",
      "features should be used by the model, their type (e.g. continuous,\n",
      "categorical) and metadata (e.g. cardinality).\n",
      "3.1.2 Model training and evaluation. The HF Transformers library\n",
      "provides its own optimized training and evaluation pipeline for\n",
      "NLP tasks, which is managed by the Trainer class. With Trans-\n",
      "formers4Rec we inherit from this class and specifically override the\n",
      "predict() andevaluate() methods to adapt them to the recommenda-\n",
      "tion problem, keeping its original train() method, as it is identical\n",
      "for NLP and sequential recommendation. The Transformers4Rec\n",
      "Meta-Architecture, detailed in Section 3.2, is a highly configurable\n",
      "component which defines the computational graph for features\n",
      "2A TensorFlow implementation of Transformers4Rec is planned for a near future, as\n",
      "HF Transformers also offers a TF version.\n",
      "3https://github.com/NVIDIA/NVTabular/processing, sequence masking and processing with Transformers,\n",
      "prediction heads and loss functions.\n",
      "The evaluation of sequential recommendation and session-based\n",
      "recommendation is performed using traditional Top-N ranking met-\n",
      "rics such as NDCG@N, Recall@N, Precision@N, MAP@N, among\n",
      "others, which are logged to a diverse number of formats as shown in\n",
      "the Outputs section of Figure 2. The library supports an incremen-\n",
      "tal training and evaluation protocol [ 39,40,51], which emulates\n",
      "realistic production scenario where the RecSys model is retrained\n",
      "(fine-tuned) with streaming data within a specified frequency (e.g.\n",
      "once a day, once an hour) and deployed for inference of the sessions\n",
      "from the next time period. More details on this protocol are found\n",
      "in Section 4.1.\n",
      "3.2 Transformers4Rec Meta-Architecture\n",
      "The Transformers4Rec’s Meta-Architecture is presented in Figure 3.\n",
      "Input features, which can be sparse categorical features or con-\n",
      "tinuous numerical features, are normalized and combined by the\n",
      "Features Processing module , which produces the interaction embed-\n",
      "ding. The sequence of the interaction embeddings is then masked\n",
      "by the Sequence Masking module according to the training approach\n",
      "(e.g., Causal LM, Masked LM) and fed to the Sequence Processing\n",
      "module , which contains stacked Transformer blocks, whose number\n",
      "of blocks and architecture type (e.g. GPT-2, Transformer-XL, XLNet,\n",
      "Electra) are also configurable. It outputs a vector for each position\n",
      "in the sequence, which is then projected to represent a sequence\n",
      "embedding . Finally, the Prediction head module can be configured\n",
      "for different tasks: items prediction (for item recommendation) or\n",
      "sequence-level predictions (classification or regression).\n",
      "In this paper experiments we used the items prediction head .\n",
      "It was composed by an output layer using the tying embeddings\n",
      "technique (see Section 3.2.3) i.e., weight-tying the projection layer\n",
      "to the item embedding matrix weights, followed by a softmax layer\n",
      "to predict the relevance scores over all items. Cross-entropy loss\n",
      "was used, but other pairwise losses functions are available. Some\n",
      "options and techniques of the meta-architecture are described next.RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Figure 3: Transformers4Rec neural meta-architecture\n",
      "3.2.1 Input Feature Representation. NLP models represent words\n",
      "and sub-words as token ids. Similarly, in RecSys the item id is the\n",
      "most important to represent the interaction, but many other fea-\n",
      "tures are generally available to provide additional information from\n",
      "item metadata and user context. The Transformers4Rec supports\n",
      "multiple interaction-level features, that can be normalized and com-\n",
      "bined in different ways. The size of categorical embeddings can be\n",
      "proportional to the features cardinality or have a fixed size. Contin-\n",
      "uous numerical features can be represented as a real-valued scalar,\n",
      "as a linear projection from a scalar, or as a Soft One-Hot Encoding\n",
      "(SOHE) [27], described in our Online Appendix A4[1].\n",
      "3.2.2 Input Feature Aggregation. Two different aggregation func-\n",
      "tions are available in our framework: (a) concatenation merge and\n",
      "(b) element-wise merge. Concatenation merge was used in the\n",
      "fusing sequence-level features with the items approach of [ 38], in\n",
      "which they include sequence-level and user level-features as in-\n",
      "teraction features and explore both concatenation and element-\n",
      "wise merge. Each session or user sequence s(u)is represented\n",
      "by a sequence of nuitems, x(u)=x(u)\n",
      "1:nuandIfeature sequences\n",
      "f(u)={f(u)\n",
      "i,1:nu:i∈1, ...,I}.\n",
      "The concatenation merge consists in simply concatenating the\n",
      "item id feature x(u)\n",
      "kwith the other available input features for the in-\n",
      "teraction at position kas follows: mk=concat(x(u)\n",
      "k,f(u)\n",
      "1,k, ...,f(u)\n",
      "I,k).\n",
      "In an element-wise merge the additional features are first element-\n",
      "wise summed and then element-wise multiplied by the item em-\n",
      "bedding as follows: mk=x(u)\n",
      "k⊙[1+f(u)\n",
      "1,k+...+f(u)\n",
      "I,k].\n",
      "In order to achieve this the embeddings of item id and all addi-\n",
      "tional features must share the same dimension. In our library we\n",
      "support the element-wise merge approach proposed by [ 4], which\n",
      "adds 1to the resulting summation of additional features embed-\n",
      "dings. As those embeddings are randomly initialized by 0-mean\n",
      "4https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_\n",
      "transformers4recGaussian distributions, the multiplicative term will have mean 1 and\n",
      "can act as a mask/attention mechanism over the item embedding.\n",
      "3.2.3 Tying Embeddings. The NLP community commonly uses a\n",
      "technique that ties the input embedding weights with the output\n",
      "projection layer matrix [ 22,46], motivated by the fact that input and\n",
      "output of models are in the same space (words). For RecSys, a par-\n",
      "ticularly important benefit is the reduction of model parameters, as\n",
      "the number of embeddings of high-cardinality categorical features\n",
      "is much larger than in NLP. Having this explicit weight-tying also\n",
      "helps the network to regularize; in particular rare item embeddings\n",
      "benefit from the more direct output layer updates at each training\n",
      "step. Perhaps most importantly in the RecSys use case, tying embed-\n",
      "dings introduces a matrix factorization operation between the item\n",
      "embeddings and the final representation of the user or session. This\n",
      "technique was originally adapted from NLP to RecSys in [ 17] and\n",
      "demonstrated to be particularly effective in the NVIDIA.AI team\n",
      "solution for Booking.com Challenge [ 48]. A formal description of\n",
      "thetying embeddings technique is available in our online Appendix\n",
      "A[1]. In early experiments, we found out that tying embeddings\n",
      "improved the NDCG@20 of both GRU and of all Transformer archi-\n",
      "tectures, by an average of 6.7% for an e-commerce dataset and 1%\n",
      "for a news dataset. Thus we enabled tying embeddings by default\n",
      "for all experiments reported in this paper and strongly suggest its\n",
      "use in neural-based sequential recommendation architectures.\n",
      "3.2.4 Regularization. Our Meta-Architecture supports a number\n",
      "of regularization techniques like Dropout [ 49], Weight Decay [ 30],\n",
      "Softmax Temperature Scaling [ 16], Layer Normalization[ 2], Sto-\n",
      "chastic Shared Embeddings [ 58], and Label Smoothing [ 52]. In our\n",
      "experiments we optimize all regularization techniques through the\n",
      "hyperparameter optimization carried out in our empirical evalua-\n",
      "tion. While many methods failed to make a significant difference,\n",
      "we found that the Label Smoothing was particularly useful at im-\n",
      "proving both train and validation accuracy. This technique was also\n",
      "shown to improve models generalization and calibration in [43].Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "3.2.5 Loss functions. Our framework supports training with many\n",
      "different loss functions, including cross-entropy (XE) and the follow-\n",
      "ing pairwise losses: BPR , TOP1 [ 19], BPR-max and TOP1-max [ 17].\n",
      "In the experimentation for this paper we do not explore different\n",
      "loss functions, but sitck with a single loss function for our empirical\n",
      "analysis (cross-entropy) as we explore many other variables.\n",
      "3.2.6 Extensibility. The Meta-Architecture modules are regular Py-\n",
      "Torch modules5that can be combined or replaced with other custom\n",
      "modules. It is possible for example to have multiple input sequences\n",
      "being processed by separate Transformer architectures and then\n",
      "having the outputs of those towers combined. Multi-task learning\n",
      "is also enabled by creating a custom prediction head combining\n",
      "multiple loss functions.\n",
      "4 EMPIRICAL ANALYSIS OF TRANSFORMERS\n",
      "FOR SESSION-BASED RECOMMENDATION\n",
      "In this section, we present an empirical analysis of different Trans-\n",
      "former architectures and training approaches for the task of session-\n",
      "based recommendation. We describe our methodology and discuss\n",
      "our research questions results.\n",
      "4.1 Methodology\n",
      "This section describes the training and evaluation protocols, includ-\n",
      "ing metrics and the hyperparameter tuning process used for the\n",
      "experiments. We also describe the datasets and their preprocessing.\n",
      "4.1.1 Incremental Training and Evaluation. Online services receive\n",
      "a continuous stream of user interactions which makes the available\n",
      "dataset larger every day [ 65]. This scenario increases the time and\n",
      "computational resources required to train models and presents en-\n",
      "gineering challenges for large-scale recommender systems. In this\n",
      "work, as in [ 39,40,51,65], our experiments are performed using\n",
      "incremental retraining. For each algorithm, a sliding window with\n",
      "a single time unit (e.g. day or hour) is provided in temporal order\n",
      "and only once, to train the algorithms incrementally. For a near-\n",
      "est neighbor algorithm like V-SkNN, STAN or VSTAN (described\n",
      "in Section 4.1.5), this means that the algorithm needs to keep in\n",
      "memory a sample of sessions from past time windows. For neural\n",
      "networks, this means fine-tuning the parameters of a model already\n",
      "trained with past data.\n",
      "Experiments are performed using incremental evaluation, as\n",
      "done in [ 39,40,51], which allows us to emulate a common produc-\n",
      "tion environment scenario where the recommendation algorithms\n",
      "are continuously trained and deployed once a day or even once\n",
      "an hour to serve recommendations for the next time period. For\n",
      "our experiments sessions are split into time windows T, with a\n",
      "length of one day for ecommerce datasets and one hour for news\n",
      "datasets. Evaluation is performed for each next time window Ti+1\n",
      "with i∈[1, . . . , n−1], using sessions from past time windows for\n",
      "training[T1, . . . , Ti]. The sessions of each time window are split\n",
      "50:50 between validation and test set. The time window validation\n",
      "sets are used for hyperparameter tuning and test sets for reporting\n",
      "metrics. The final reported metrics are the average of five indepen-\n",
      "dent runs with different random seeds, using the best configuration\n",
      "found during the hyperparameter optimization process. For each\n",
      "5Or Keras layers in the future TensorFlow version of Transformers4Rec.run, the metrics are the averages of all time windows, i.e., Aver-\n",
      "age over Time (AoT) , to benefit algorithms that are consistent at\n",
      "providing accurate recommendations over time.\n",
      "4.1.2 Hyperparameter optimization. For each set of experiment\n",
      "group, composed by an algorithm, training approach and dataset,\n",
      "we perform bayesian hyperparameter optimization for 100 trials\n",
      "- running five in parallel - and optimizing towards maximizing\n",
      "NDCG@20 of the validation set. To reduce the possibility of overfit-\n",
      "ting over specific days, the hyperparameter tuning process performs\n",
      "incremental training and evaluation only on the first 50% of the\n",
      "available days for each dataset (see Table 1) and the reported metrics\n",
      "are computed on the test set for all available days. In our results we\n",
      "report the average of five runs using the best hyperparameters. Our\n",
      "Online Appendix C [ 1] details the hyperparameters search space\n",
      "explored in our experiments, along with their best values found.\n",
      "4.1.3 Metrics. We evaluate the algorithms for their ability to pre-\n",
      "dict the last interacted item in a session. As sessions lengths range\n",
      "between 2 and 20 after preprocessing, this task is equivalent to\n",
      "next-click prediction in the sense that recommendations for all\n",
      "positions of sequences are represented in the evaluation. The fol-\n",
      "lowing information retrieval metrics are used to compute Top-20\n",
      "accuracy of recommendation lists containing all items: Normalized\n",
      "Discounted Cumulative Gain (NDCG@20) and Hit Rate (HR@20) ,\n",
      "which is equivalent to Recall@n when there is only one relevant\n",
      "item in the recommendation list. NDCG accounts for rank of the\n",
      "relevant item in the recommendation list and is a more fine-grained\n",
      "metric than HR, which only verifies whether the relevant item is\n",
      "among the top-n items. Because of this we optimize for NDCG@20\n",
      "in our hyperparameter tuning and consider it to be the primary\n",
      "metric in our study.\n",
      "4.1.4 Datasets and Preprocessing. We have selected for experi-\n",
      "ments the e-commerce and news domains, where session-based\n",
      "recommendation is very suitable. In the news domain, many users\n",
      "browse anonymously with only their last interactions available. In\n",
      "the e-commerce domain, besides the user cold-start problem, user\n",
      "sessions tend to be targeted to a specific purchase need, so inter-\n",
      "actions from the current session provide more useful information\n",
      "than past interactions for the user context. We have selected two\n",
      "datasets for each of the domains, described as follows:\n",
      "•REES46 eCommerce6- This dataset contains seven months\n",
      "of user session from a multi-category online store, including\n",
      "events like views, add-to-card and purchase events. As this\n",
      "is a large dataset, we use in our experiments only events\n",
      "from the month of Oct. 2019.\n",
      "•YOOCHOOSE eCommerce7- This dataset was released for\n",
      "the RecSys Challenge 2015 and is composed by clicks and\n",
      "buying events from user sessions on e-commerce. We use\n",
      "only the use interactions table, as they form the majority of\n",
      "data and we are interested in next-click prediction.\n",
      "6https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-\n",
      "category-store\n",
      "7https://2015.recsyschallenge.com/challenge.htmlRecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Table 1: Dataset statistics\n",
      "Dataset days items\n",
      "(K)sessions\n",
      "(M)interactions\n",
      "(M)sessions\n",
      "length\n",
      "(avg.)Gini\n",
      "index\n",
      "REES46\n",
      "eCommerce31 156,516 3,268,268 17,967,918 5.49 0.86\n",
      "YOOCHOOSE\n",
      "eCommerce182 50,549 6,756,575 26,478,390 3.83 0.89\n",
      "G1 news 16 46,027 1,048,556 2,988,037 2.84 0.94\n",
      "ADRESSA\n",
      "news16 13,820 982,210 2,648,999 2.69 0.96\n",
      "•G1 news8- This dataset [ 39–41] was shared by globo.com,\n",
      "the most popular media company in Brazil. It contains sam-\n",
      "pled user sessions with page views of the G1 news portal\n",
      "during a period of 16 days . It also provides metadata and a\n",
      "vectorial representation of the textual content of the news\n",
      "articles interacted during that period.\n",
      "•ADRESSA news9- This news dataset [ 15] comes from col-\n",
      "laboration of the NTNU and Adressavisen from Norway. It\n",
      "includes both page views and the textual content and meta-\n",
      "data of news articles. We use only the first 16 days of this\n",
      "dataset, so that its is comparable with the G1 news dataset.\n",
      "Table 1 shows the statistics of these preprocessed datasets. It\n",
      "can be seen that the e-commerce datasets are larger than the news\n",
      "datasets in all statistics, and in particular the REES46 dataset has\n",
      "more sessions available per day and has longer avg. session length.\n",
      "The statistics of the G1 and Adressa news dataset are very similar in\n",
      "general. Finally, the Gini index of the items frequency distribution\n",
      "shows that the news dataset are more long-tailed, showing higher\n",
      "popularity bias with interactions more concentrated in a smaller\n",
      "set of very popular items.\n",
      "To prepare the data, user interactions are grouped by sessions and\n",
      "consecutive repeated interactions are removed. We ignore sessions\n",
      "with length 1, and truncate sessions up to the maximum of 20\n",
      "interactions. The sessions are divided in time windows, according\n",
      "to the unit: one day for e-commerce datasets and one hour for\n",
      "the news datasets, which are more dynamic. We also explore the\n",
      "usage of side features by Transformers architectures (RQ3). The full\n",
      "description of the preprocessing and feature engineering techniques\n",
      "is available in our Online Appendix B[1].\n",
      "4.1.5 Baseline Algorithms. In a recent empirical analysis on session-\n",
      "based recommendation [ 35], which extensively evaluated the per-\n",
      "formance of 12 algorithms across 8 datasets, the top 4 algorithms\n",
      "ordered by their average ranking by dataset were: STAN [ 13], SKNN\n",
      "[23], V-SkNN [ 31] and VSTAN [ 35]. Interestingly, algorithms based\n",
      "on Session-based k-Nearest Neighbors provided more accurate rec-\n",
      "ommendations than other neural architectures like GRU4REC [ 18],\n",
      "NARM [ 26], STAMP [ 28] and SR-GNN [ 60], a phenomena also\n",
      "observed in previous research [23, 31, 32].\n",
      "We have included in our analysis four baseline algorithms: V-\n",
      "SkNN [ 31], STAN [ 13], VSTAN [ 35] and GRU4Rec [ 18]. In addition,\n",
      "we used the original Gated Recurrent Unit (GRU) [ 7] layers as a\n",
      "replacement to the Transformer blocks in the Session Processing\n",
      "module of our proposed Meta-Architecture (Figure 3) to allow us to\n",
      "isolate the potential improvements obtained by the usage of Trans-\n",
      "formers compared to an RNN. Session-based k-NN algorithms can\n",
      "8https://www.kaggle.com/gspmoreira/news-portal-user-interactions-by-globocom\n",
      "9http://reclab.idi.ntnu.no/dataset/be trained incrementally like the Transformers4Rec framework10.\n",
      "In the case of the GRU4Rec implementation, which does not support\n",
      "incremental training, we used two approaches to train individual\n",
      "models for each evaluation time window Ti: (1)Full Training (FT) -\n",
      "Trains a model on all available time windows prior to the evaluation\n",
      "window from T1toTi−1, and (2) Sliding Window Training (SWT) -\n",
      "Trains a model with the last wtime windows, from Ti−wtoTi−1.\n",
      "We defined the sliding window size was 20% of the number of days\n",
      "available for each dataset, i.e. 6 days for REES46 eCommerce, 36\n",
      "days for YOOCHOOSE, and 3 days for both G1 and ADRESSA.\n",
      "4.2 RQ1: Can transformer-based architectures\n",
      "provide accurate next-click predictions for\n",
      "the shorter user sequences found in the\n",
      "session-based recommendation task?\n",
      "Transformers have been shown to outperform RNNs for long se-\n",
      "quences in both NLP and sequential recommendation task, however\n",
      "as discussed in Section 2.3, the average session length (see Table 1)\n",
      "is much shorter in a session-based setting. In this section we explore\n",
      "the effectiveness of Transformers for session-based recommenda-\n",
      "tion. The results for all research questions are shown in Table 2\n",
      "for easy comparison. For each research question, the best results\n",
      "for a metric are printed in bold and marked with a * if they signif-\n",
      "icantly11outperform all other algorithms. The baselines for each\n",
      "research question are highlighted in light gray, and for RQ1 the\n",
      "best performing baselines are underlined.\n",
      "For RQ1 we focus on the results of Transformers architectures\n",
      "trained with their original training approaches (under parenthesis)\n",
      "relative to the neural and non-neural baselines. From Table 2 we see\n",
      "that the Session k-NN algorithms (V-SkNN, STAN, and VSTAN) are\n",
      "indeed strong baselines for session-based recommendation, with\n",
      "higher HR@20 than some of the Transformer architectures, how-\n",
      "ever GRU4REC is the best baseline for both e-commerce datasets in\n",
      "terms of NDCG@20, under both FT and SWT training approaches.\n",
      "For the news datasets GRU4REC underperforms compared to the\n",
      "Session k-NN methods, which was also observed in [ 39], however\n",
      "GRU is the best performing baseline for the ADRESSA news on\n",
      "both NDCG@20 and HR@20, and among the best for G1 news.\n",
      "Turning our attention to the Transformer architecture results we\n",
      "see that they achieve the best NDCG@20 across all four datasets,\n",
      "but by a much larger margin (+8.95% NDCG) on both e-commerce\n",
      "datasets than on the news datasets. We believe that this is due to\n",
      "the fact that the e-commerce sessions are longer than than news\n",
      "sessions (Table 1), requiring more complex models. No single Trans-\n",
      "former architecture or training technique performs best across all\n",
      "datasets raising the question of how to select the appropriate algo-\n",
      "rithm without the extensive experimentation done in this paper. In\n",
      "RQ2 we attempt to isolate the effect of training technique by train-\n",
      "ing the XLNet architecture with the available options, producing a\n",
      "clearer choice.\n",
      "10To make V-SkNN and VSTAN support incremental training, we updated their imple-\n",
      "mentations to compute the IDFstatistics incrementally\n",
      "11As errors around the reported averages were normally distributed, we used paired\n",
      "Student’s t-tests with p<0.05and Bonferroni correction for significance tests.Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "Table 2: Experimental Results: RQ1 / RQ2 / RQ3\n",
      "REES46 eCommerce YOOCHOOSE eCommerce G1 news ADRESSA news\n",
      "Algorithm NDCG@20 HR@20 NDCG@20 HR@20 NDCG@20 HR@20 NDCG@20 HR@20\n",
      "RQ1 V-SkNN 0.2187 0.4662 0.2975 0.5110 0.3511 0.6601 0.3590 0.7210\n",
      "STAN 0.2194 0.4797 0.3082 0.5196 0.3570 0.6681 0.3635 0.7246\n",
      "VSTAN 0.2200 0.4857 * 0.3097 0.5206 0.3586 0.6668 0.3617 0.7241\n",
      "GRU4Rec (FT) 0.2231 0.4414 0.3442 0.5891 0.2596 0.5029 0.3007 0.6052\n",
      "GRU4Rec (SWT) 0.2204 0.4359 0.3431 0.5885 0.2666 0.5183 0.2967 0.5948\n",
      "GRU (CLM) 0.2139 0.4315 0.2975 0.6129 0.3549 0.6632 0.3799 0.7413\n",
      "GPT-2 (CLM) 0.2165 0.4338 0.2975 0.6065 0.3560 0.6620 0.3790 0.7398\n",
      "Transformer-XL (CLM) 0.2197 0.4404 0.3585 0.6133 0.3294 0.6192 0.3811* 0.7382\n",
      "BERT (MLM) 0.2218 0.4672 0.3750* 0.6349* 0.3549 0.6549 0.3725 0.7221\n",
      "ELECTRA (RTD) 0.2430 0.4768 0.3722 0.6294 0.3588 0.6600 0.3729 0.7226\n",
      "XLNet (PLM) 0.2422 0.4760 0.3681 0.6282 0.3551 0.6634 0.3673 0.7212\n",
      "RQ2 XLNet (PLM) - original 0.2422 0.4760 0.3681 0.6282 0.3551 0.6634* 0.3673 0.7212\n",
      "XLNet (CLM) 0.2108 0.4219 0.3557 0.6079 0.3551 0.6508 0.3770 0.7378*\n",
      "XLNet (RTD) 0.2546* 0.4886* 0.3776 0.6373 0.3609 0.6611 0.3816 0.7329\n",
      "XLNet (MLM) 0.2428 0.4763 0.3776 0.6384* 0.3607 0.6605 0.3822 0.7349\n",
      "RQ3 XLNet (MLM) - item id 0.2428 0.4763 0.3776 0.6384 0.3607 0.6605 0.3822 0.7349\n",
      "Concat. merge 0.2522 0.4782 - - 0.3652 0.6714 0.3912* 0.7488*\n",
      "Concat. merge + SOHE 0.2542* 0.4858 - - 0.3675* 0.6721* 0.3886 0.7463\n",
      "Element-wise merge 0.2529 0.4854 - - 0.3614 0.6678 0.3892 0.7433\n",
      "4.3 RQ2: How do the training techniques of\n",
      "Causal LM, Masked LM, Permutation LM,\n",
      "and Replacement Token Detection perform\n",
      "comparatively for the task of session-based\n",
      "recommendation?\n",
      "RQ1 shows the effectiveness of Transformer-based language models\n",
      "for session-based recommendation, however these language models\n",
      "include two components: (1) the pre-training approach and (2) the\n",
      "Transformer architecture. In RQ2 we wanted to isolate the gain of\n",
      "performance related to the different training approaches and com-\n",
      "pare their effectiveness when considering the short sequences of the\n",
      "session-based recommendation task. The XLNet architecture was\n",
      "designed to leverage the best of both auto-regressive language mod-\n",
      "eling and auto-encoding with its Permutation Language Modeling\n",
      "training method. It can also be used with MLM and RTD, so in RQ2\n",
      "we used XLNet as the Transformer block in our meta-architecture\n",
      "and trained the model with three methods different from its original\n",
      "PLM: CLM, MLM and RTD, introduced in Section 2.1.\n",
      "Table 2 shows the results of the XLNet model when trained with\n",
      "different approaches. First, we notice that MLM outperforms CLM\n",
      "for all datasets. This could be explained by the fact that the former\n",
      "has access to future in-session interactions (privileged information)\n",
      "during training and therefore better represents the context of the\n",
      "items. In addition, MLM randomly masks different positions of\n",
      "the session for each training step, working as a data augmenta-\n",
      "tion approach. For both PLM and MLM, partial prediction plays a\n",
      "role of reducing optimization difficulty by only predicting tokens\n",
      "with sufficient context. Interestingly, MLM outperforms the orig-\n",
      "inal pre-training approach used by the XLNet model (PLM) for\n",
      "all datasets on NDCG@20, suggesting that MLM performs better\n",
      "when context is limited. Finally, XLNet (RTD) extends XLNet (MLM)\n",
      "with the additional RTD task that learns from all the items in the\n",
      "sequence and therefore provides more effective training signals.\n",
      "From Table 2 we see that XLNet (RTD) has the highest NDCG@20\n",
      "on three datasets of all combinations explored in both RQ2 and\n",
      "RQ1 and achieves 99.69% of the best performing algorithm for both\n",
      "NDCG@20 and HR@20 across all datasets, making it an excellent\n",
      "default choice when hyperparameter tuning across all architecturesand training techniques isn’t an option. It improves the benefit from\n",
      "Transformers further on REES46 e-commerce and YOOCHOOSE e-\n",
      "commerce (+14.15% NDCG@20 and +9.75% NDCG@20 respectively\n",
      "relative to the best baseline), and by a small amount on G1 news,\n",
      "where ELECTRA (also trained on RTD) achieved the best results\n",
      "in RQ1. Our finding is particularly interesting as this combination\n",
      "of Transformer architecture and training technique - XLNet (RTD)\n",
      "- is (to our knowledge) novel and could be beneficial to the NLP\n",
      "community as well.\n",
      "4.4 RQ3: What are effective approaches to\n",
      "integrate additional features, commonly\n",
      "referred to as side information, into\n",
      "transformer architectures in order to\n",
      "improve recommendation accuracy?\n",
      "It is a common practice in RecSys to leverage additional tabular fea-\n",
      "tures of item metadata and user context, providing the model more\n",
      "information for meaningful predictions. In this research question,\n",
      "we have compared three different approaches to include side infor-\n",
      "mation into Transformer architectures: (1) Concatenation merge,\n",
      "(2) Concatenation merge with continuous features represented by\n",
      "the Soft-One Hot Encoding (SOHE) technique, and (3) Element-wise\n",
      "merge, described in Section 3.2.2. We empirically observed that ap-\n",
      "plying layer normalization technique[ 2] individually to each feature\n",
      "representation before merging was essential to obtain improve-\n",
      "ments in accuracy when including continuous features, aligned\n",
      "with what we also observed in [ 42], so we used this normaliza-\n",
      "tion technique. In Online Appendix B [ 1], we describe the feature\n",
      "engineering steps and why the YOOCHOOSE dataset was not in-\n",
      "cluded in this analysis. We chose for this analysis XLNet (MLM),\n",
      "based on its performance for RQ2 over all datasets, leaving as future\n",
      "work whether XLNet (RTD) could futher improve performance. All\n",
      "proposed techniques that include additional features improve the\n",
      "performance of XLNet (MLM) compared to using only the item id\n",
      "feature; in Table 2 we see a relative improvement of NDCG@20 of\n",
      "2.13% on average for the news datasets and of 4.72% for the REES46\n",
      "e-commerce dataset which includes more informative features. Con-\n",
      "catenation Merge using Soft-One Hot Encoding (SOHE) providedRecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "the highest NDCG@20 and HR@20 for REES46 e-commerce and\n",
      "G1 news, suggesting that the SOHE technique is able to provide\n",
      "effective representation of continuous features to be combined with\n",
      "categorical embeddings. For ADRESSA news, the Concatenation\n",
      "Merge performed the best, which was similarly observed in [38].\n",
      "5 CONCLUSION\n",
      "In this paper we have presented Transformers4Rec, an open source\n",
      "library designed to enable RecSys researchers and practitioners to\n",
      "quickly and easily explore the latest developments of the NLP for\n",
      "sequential and session-based recommendation tasks. Our contri-\n",
      "butions are threefold. First, to motivate our efforts we provided\n",
      "an overview of the relationship between the fields of NLP and\n",
      "sequential / session-based recommendation, highlighting the corre-\n",
      "sponding algorithms developed in each field. Second, the library\n",
      "itself, which wraps HF Transformers, providing all of the neces-\n",
      "sary functionality required to use Transformer architectures in a\n",
      "RecSys setting. Third, we have performed experiments on the ap-\n",
      "plicability of modern NLP Transformer architectures to the task of\n",
      "session-based recommendation, comparing CLM, MLM, PLM, and\n",
      "RTD as training techniques for session-based recommendation for\n",
      "XLNet, and examining three methods of adding side information to\n",
      "Transformers.\n",
      "Through our experiments for the session-based recommendation\n",
      "task we have found that Transformer architectures have superior\n",
      "performance (+14.15% NDCG@20 and +9.75% NDCG@20 respec-\n",
      "tively relative to the best baseline) across REES46 and YOOCHOOSE\n",
      "e-commerce datasets while performing similarly to baselines on\n",
      "G1 and ADRESSA news datasets. In particular, we discovered that\n",
      "XLNet trained with RTD, a novel combination of Transformer archi-\n",
      "tecture and training technique proposed in this paper, is effective at\n",
      "the task of session-based recommendation with the best NDCG@20\n",
      "on three datasets and achieving 99.69% of the best performing algo-\n",
      "rithm for both NDCG@20 and HR@20 across all datasets. Finally,\n",
      "by exploring different methods to leverage side information with\n",
      "Transformers, we found that aggregation by concatenation out-\n",
      "performed using only the item id feature by +4.72% NDCG@20\n",
      "(+13.96% relative to the best popular baseline) on REES46 eCom-\n",
      "merce and by +2.13% NDCG@20 (+2.75% relative to the best base-\n",
      "line) averaged across G1 and ADRESSA news for the XLNet (MLM)\n",
      "architecture suggesting that it is a valuable addition to session-\n",
      "based recommenders. We hope that these experiments encourage\n",
      "more exploration into the use of Transformers within the RecSys\n",
      "domain and that the Transformers4Rec library helps to foster an\n",
      "easier exchange of ideas and research between the NLP and RecSys\n",
      "communities.\n",
      "REFERENCES\n",
      "[1]2021. Transformers4Rec paper online appendix . https://github.com/NVIDIA-\n",
      "Merlin/publications/tree/main/2021_acm_recsys_transformers4rec\n",
      "[2]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\n",
      "tion. arXiv preprint arXiv:1607.06450 (2016).\n",
      "[3]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2016. Neural Machine\n",
      "Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs.CL]\n",
      "[4]Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H\n",
      "Chi. 2018. Latent cross: Making use of context in recurrent recommender systems.\n",
      "InProceedings of the Eleventh ACM International Conference on Web Search and\n",
      "Data Mining . 46–54.\n",
      "[5]Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior\n",
      "sequence transformer for e-commerce recommendation in alibaba. In Proceedingsof the 1st International Workshop on Deep Learning Practice for High-Dimensional\n",
      "Sparse Data . 1–4.\n",
      "[6]Xusong Chen, Dong Liu, Chenyi Lei, Rui Li, Zheng-Jun Zha, and Zhiwei Xiong.\n",
      "2019. BERT4SessRec: Content-Based Video Relevance Prediction with Bidirec-\n",
      "tional Encoder Representations from Transformer. In Proceedings of the 27th ACM\n",
      "International Conference on Multimedia . 2597–2601.\n",
      "[7]Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,\n",
      "Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\n",
      "representations using RNN encoder-decoder for statistical machine translation.\n",
      "arXiv preprint arXiv:1406.1078 (2014).\n",
      "[8]Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.\n",
      "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\n",
      "arXiv:2003.10555 [cs.CL]\n",
      "[9]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan\n",
      "Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a\n",
      "Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association\n",
      "for Computational Linguistics .\n",
      "[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\n",
      "Pre-training of Deep Bidirectional Transformers for Language Understanding.\n",
      "InProceedings of the 2019 Conference of the North American Chapter of the Asso-\n",
      "ciation for Computational Linguistics: Human Language Technologies, Volume 1 .\n",
      "Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186.\n",
      "[11] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning\n",
      "for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations.\n",
      "ACM Transactions on Information Systems (TOIS) 39, 1 (2020), 1–42.\n",
      "[12] Jun Fang. 2021. Session-based Recommendation with Self-Attention Networks.\n",
      "arXiv preprint arXiv:2102.01922 (2021).\n",
      "[13] Diksha Garg, Priyanka Gupta, Pankaj Malhotra, Lovekesh Vig, and Gautam Shroff.\n",
      "2019. Sequence and time aware neighborhood for session-based recommenda-\n",
      "tions: Stan. In Proceedings of the 42nd International ACM SIGIR Conference on\n",
      "Research and Development in Information Retrieval . 1069–1072.\n",
      "[14] Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan Bhamidipati,\n",
      "Jaikit Savla, Varun Bhagwan, and Doug Sharp. 2015. E-commerce in your inbox:\n",
      "Product recommendations at scale. In Proceedings of the 21th ACM SIGKDD\n",
      "international conference on knowledge discovery and data mining . 1809–1818.\n",
      "[15] Jon Atle Gulla, Lemei Zhang, Peng Liu, Özlem Özgöbek, and Xiaomeng Su. 2017.\n",
      "The adressa dataset for news recommendation. In Proceedings of the international\n",
      "conference on web intelligence . 1042–1048.\n",
      "[16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration\n",
      "of modern neural networks. In International Conference on Machine Learning .\n",
      "PMLR, 1321–1330.\n",
      "[17] Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with\n",
      "top-k gains for session-based recommendations. In Proceedings of the 27th ACM\n",
      "international conference on information and knowledge management . 843–852.\n",
      "[18] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n",
      "2015. Session-based recommendations with recurrent neural networks. arXiv\n",
      "preprint arXiv:1511.06939 (2015).\n",
      "[19] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n",
      "2016. Session-based recommendations with recurrent neural networks. In Pro-\n",
      "ceedings of Fourth International Conference on Learning Representations (ICLR’16) .\n",
      "[20] Balázs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos\n",
      "Tikk. 2016. Parallel recurrent neural network architectures for feature-rich\n",
      "session-based recommendations. In Proceedings of the 10th ACM conference on\n",
      "recommender systems . 241–248.\n",
      "[21] Xiaowen Huang, Shengsheng Qian, Quan Fang, Jitao Sang, and Changsheng Xu.\n",
      "2018. Csan: Contextual self-attention network for user sequential recommen-\n",
      "dation. In Proceedings of the 26th ACM international conference on Multimedia .\n",
      "447–455.\n",
      "[22] Hakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors\n",
      "and word classifiers: A loss framework for language modeling. arXiv preprint\n",
      "arXiv:1611.01462 (2016).\n",
      "[23] Dietmar Jannach and Malte Ludewig. 2017. When recurrent neural networks\n",
      "meet the neighborhood for session-based recommendation. In Proceedings of the\n",
      "Eleventh ACM Conference on Recommender Systems (RecSys’17) . 306–310.\n",
      "[24] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\n",
      "mendation. In 2018 IEEE International Conference on Data Mining (ICDM) . IEEE,\n",
      "197–206.\n",
      "[25] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences\n",
      "and Documents. arXiv:1405.4053 [cs.CL]\n",
      "[26] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, and Jun Ma. 2017. Neural\n",
      "Attentive Session-based Recommendation. arXiv:1711.04725 [cs.IR]\n",
      "[27] Yang Li, Nan Du, and Samy Bengio. 2017. Time-dependent representation for\n",
      "neural event sequence prediction. arXiv preprint arXiv:1708.00065 (2017).\n",
      "[28] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-\n",
      "term attention/memory priority model for session-based recommendation. In\n",
      "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\n",
      "Discovery & Data Mining . 1831–1839.Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation RecSys ’21, September 27-October 1, 2021, Amsterdam, Netherlands\n",
      "[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n",
      "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\n",
      "Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n",
      "[30] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\n",
      "In7th International Conference on Learning Representations, ICLR 2019, New Or-\n",
      "leans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?\n",
      "id=Bkg6RiCqY7\n",
      "[31] Malte Ludewig and Dietmar Jannach. 2018. Evaluation of session-based recom-\n",
      "mendation algorithms. User Modeling and User-Adapted Interaction 28, 4-5 (2018),\n",
      "331–390.\n",
      "[32] Malte Ludewig, Noemi Mauro, Sara Latifi, and Dietmar Jannach. 2019. Per-\n",
      "formance comparison of neural and non-neural approaches to session-based\n",
      "recommendation. In Proceedings of the 13th ACM conference on recommender\n",
      "systems . 462–466.\n",
      "[33] Anjing Luo, Pengpeng Zhao, Yanchi Liu, Fuzhen Zhuang, Deqing Wang, Jiajie Xu,\n",
      "Junhua Fang, and Victor S Sheng. [n.d.]. Collaborative Self-Attention Network\n",
      "for Session-based Recommendation. ([n. d.]).\n",
      "[34] Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wil-\n",
      "fred Ng. 2019. SDM: Sequential deep matching model for online large-scale\n",
      "recommender system. In Proceedings of the 28th ACM International Conference on\n",
      "Information and Knowledge Management . 2635–2643.\n",
      "[35] Ludewig Malte, Noemi Mauro, Latifi Sara, Jannach Dietmar, et al .2020. Empirical\n",
      "analysis of session-based recommendation algorithms. (2020).\n",
      "[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\n",
      "Estimation of Word Representations in Vector Space. arXiv:1301.3781 [cs.CL]\n",
      "[37] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n",
      "Distributed Representations of Words and Phrases and their Compositionality.\n",
      "arXiv:1310.4546 [cs.CL]\n",
      "[38] Sarai Mizrachi and Pavel Levin. 2019. Combining Context Features in Sequence-\n",
      "Aware Recommender Systems. In RecSys (Late-Breaking Results) . 11–15.\n",
      "[39] Gabriel de Souza Pereira Moreira, Felipe Ferreira, and Adilson Marques da Cunha.\n",
      "2018. News session-based recommendations using deep neural networks. In\n",
      "Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems .\n",
      "15–23.\n",
      "[40] Gabriel De Souza Pereira Moreira, Dietmar Jannach, and Adilson Marques\n",
      "Da Cunha. 2019. Contextual hybrid session-based news recommendation with\n",
      "recurrent neural networks. IEEE Access 7 (2019), 169185–169203.\n",
      "[41] Gabriel de Souza Pereira Moreira, Dietmar Jannach, and Adilson Marques da\n",
      "Cunha. 2019. On the importance of news content representation in hybrid neural\n",
      "session-based recommender systems, In Proceedings of the 7th International\n",
      "Workshop on News Recommendation and Analytics (INRA 2019). CEUR Workshop\n",
      "Proceedings Vol-2554.\n",
      "[42] Gabriel de Souza Pereira Moreira, Sara Rabhi, Ronay Ak, Md Yasin Kabir, and\n",
      "Even Oldridge. 2021. Transformers with multi-modal features and post-fusion\n",
      "context for e-commerce session-based recommendation. In Proceedings of the\n",
      "Fifth SIGIR eCommerce Workshop 2021 .\n",
      "[43] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. 2019. When does label\n",
      "smoothing help?. In Advances in Neural Information Processing Systems , H. Wal-\n",
      "lach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (Eds.),\n",
      "Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/\n",
      "f1748d6b0fd9d439f71450117eba2725-Paper.pdf\n",
      "[44] Zhiqiang Pan, Fei Cai, Yanxiang Ling, and Maarten de Rijke. 2020. Rethinking\n",
      "Item Importance in Session-based Recommendation. In Proceedings of the 43rd\n",
      "International ACM SIGIR Conference on Research and Development in Information\n",
      "Retrieval . 1837–1840.\n",
      "[45] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe:\n",
      "Global Vectors for Word Representation. In Empirical Methods in Natural Lan-\n",
      "guage Processing (EMNLP) . 1532–1543. http://www.aclweb.org/anthology/D14-\n",
      "1162\n",
      "[46] Ofir Press and Lior Wolf. 2017. Using the Output Embedding to Improve Language\n",
      "Models. In Proceedings of the 15th Conference of the European Chapter of the\n",
      "Association for Computational Linguistics: Volume 2, Short Papers . Association for\n",
      "Computational Linguistics, Valencia, Spain, 157–163. https://www.aclweb.org/\n",
      "anthology/E17-2025\n",
      "[47] A. Radford and Karthik Narasimhan. 2018. Improving Language Understanding\n",
      "by Generative Pre-Training.\n",
      "[48] Benedikt Schifferer, Chris Deotte, Jean-Franćois Puget, Gabriel de Souza Pereira\n",
      "Moreira, Gilberto Titericz, Jiwei Liu, and Ronay Ak. 2021. Using Deep Learning\n",
      "to Win the Booking.com WSDMWebTour21 Challenge on Sequential Recommen-\n",
      "dations (to be published). https://www.bookingchallenge.com/. In Proceedings of\n",
      "the ACM WSDM Workshop on Web Tourism (WSDM WebTour’21) .\n",
      "[49] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\n",
      "Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from\n",
      "overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.\n",
      "[50] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n",
      "2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\n",
      "resentations from transformer. In Proceedings of the 28th ACM international\n",
      "conference on information and knowledge management . 1441–1450.[51] Shiming Sun, Yuanhe Tang, Zemei Dai, and Fu Zhou. 2019. Self-attention network\n",
      "for session-based recommendation with streaming data input. IEEE Access 7\n",
      "(2019), 110499–110509.\n",
      "[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\n",
      "Wojna. 2016. Rethinking the inception architecture for computer vision. In\n",
      "Proceedings of the IEEE conference on computer vision and pattern recognition .\n",
      "2818–2826.\n",
      "[53] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\n",
      "networks for session-based recommendations. In Proceedings of the 1st workshop\n",
      "on deep learning for recommender systems . 17–22.\n",
      "[54] Flavian Vasile, Elena Smirnova, and Alexis Conneau. 2016. Meta-Prod2Vec.\n",
      "Proceedings of the 10th ACM Conference on Recommender Systems (Sep 2016).\n",
      "https://doi.org/10.1145/2959100.2959160\n",
      "[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
      "Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. arXiv preprint arXiv:1706.03762 (2017).\n",
      "[56] Shoujin Wang, Longbing Cao, Yan Wang, Quan Z Sheng, Mehmet Orgun, and\n",
      "Defu Lian. 2019. A survey on session-based recommender systems. arXiv preprint\n",
      "arXiv:1902.04864 (2019).\n",
      "[57] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\n",
      "Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al .\n",
      "2019. HuggingFace’s Transformers: State-of-the-art natural language processing.\n",
      "arXiv preprint arXiv:1910.03771 (2019).\n",
      "[58] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James L. Sharpnack. 2019.\n",
      "Stochastic Shared Embeddings: Data-driven Regularization of Embedding\n",
      "Layers. In Advances in Neural Information Processing Systems 32: Annual\n",
      "Conference on Neural Information Processing Systems 2019, NeurIPS 2019,\n",
      "December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo\n",
      "Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Ro-\n",
      "man Garnett (Eds.). 24–34. https://proceedings.neurips.cc/paper/2019/hash/\n",
      "37693cfc748049e45d87b8c7d8b9aacd-Abstract.html\n",
      "[59] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT:\n",
      "Sequential recommendation via personalized transformer. In Fourteenth ACM\n",
      "Conference on Recommender Systems . 328–337.\n",
      "[60] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\n",
      "Session-based recommendation with graph neural networks. In Proceedings of\n",
      "the AAAI Conference on Artificial Intelligence , Vol. 33. 346–353.\n",
      "[61] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n",
      "2017. Attentional Factorization Machines: Learning the Weight of Feature Inter-\n",
      "actions via Attention Networks. arXiv:1708.04617 [cs.LG]\n",
      "[62] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen\n",
      "Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized Self-\n",
      "Attention Network for Session-based Recommendation.. In IJCAI , Vol. 19. 3940–\n",
      "3946.\n",
      "[63] Zhilin Yang, Zihang Dai, Yiming Yang Jaime G. Carbonell, Ruslan Salakhutdi-\n",
      "nov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for\n",
      "Language Understanding. In Advances in Neural Information Processing Systems .\n",
      "[64] Shuai Zhang, Yi Tay, Lina Yao, Aixin Sun, and Jake An. 2019. Next item recom-\n",
      "mendation with self-attentive metric learning. In Thirty-Third AAAI Conference\n",
      "on Artificial Intelligence , Vol. 9.\n",
      "[65] Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, and\n",
      "Yongdong Zhang. 2020. How to retrain recommender system? A sequential meta-\n",
      "learning method. In Proceedings of the 43rd International ACM SIGIR Conference\n",
      "on Research and Development in Information Retrieval . 1479–1488.\n",
      "[66] Yuanxing Zhang, Pengyu Zhao, Yushuo Guan, Lin Chen, Kaigui Bian, Lingyang\n",
      "Song, Bin Cui, and Xiaoming Li. 2020. Preference-aware mask for session-\n",
      "based recommendation with bidirectional transformer. In ICASSP 2020-2020 IEEE\n",
      "International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,\n",
      "3412–3416.\n",
      "[67] Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen,\n",
      "and Jun Gao. 2018. Atrank: An attention-based user behavior modeling frame-\n",
      "work for recommendation. In Proceedings of the AAAI Conference on Artificial\n",
      "Intelligence , Vol. 32.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def download_pdf_and_extract_text(pdf_url):\n",
    "    try:\n",
    "        # Télécharger le fichier PDF depuis le lien\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Vérifier si la requête a réussi\n",
    "        \n",
    "        # Déterminer le nom du fichier\n",
    "        file_name = pdf_url.split(\"/\")[-1]\n",
    "\n",
    "        # Enregistrer le PDF localement\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Extraire le texte du PDF\n",
    "        pdf_text = extract_text_from_pdf(file_name)\n",
    "        return pdf_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as ex:\n",
    "        print(f\"Une erreur inattendue s'est produite : {str(ex)}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    try:\n",
    "        # Ouvrir le fichier PDF en mode lecture binaire\n",
    "        with open(pdf_file, \"rb\") as f:\n",
    "            # Initialiser le lecteur PDF\n",
    "            pdf_reader = PdfReader(f)\n",
    "            \n",
    "            # Extraire le texte de chaque page du PDF\n",
    "            pdf_text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                pdf_text += page.extract_text()\n",
    "            \n",
    "            return pdf_text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction du texte du PDF : {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Lien PDF à télécharger\n",
    "pdf_url = \"https://scontent-cdg4-3.xx.fbcdn.net/v/t39.8562-6/246721374_422204999475172_9039387325224382577_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=e280be&_nc_ohc=f7UEYoVR9PAAb5Jmbyd&_nc_ht=scontent-cdg4-3.xx&oh=00_AfD1KZ-n5bA6jyE8wMo1rqiko8XoBL7d_vW4BgXCjybEcA&oe=662C70BA\"\n",
    "\n",
    "# Télécharger le PDF et extraire son texte\n",
    "pdf_text = download_pdf_and_extract_text(pdf_url)\n",
    "if pdf_text:\n",
    "    print(\"Texte extrait du PDF :\\n\", pdf_text)\n",
    "else:\n",
    "    print(\"Impossible de télécharger ou extraire le texte du PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcd6cf",
   "metadata": {},
   "source": [
    "# 4 Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eb0c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez saisir votre requête : I'm looking for articles on NLP\n",
      "Langue détectée ==> Anglais\n",
      "Entrez le nombre de pages à parcourir : 6\n",
      "\n",
      "Page 1\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/1902.00751.pdf\n",
      "Enregistré sous: NLP-6/article_4dcd31f3-bf7c-44e9-b02c-bc12a4f73c7b.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1355.pdf\n",
      "Enregistré sous: NLP-6/article_8fc51e08-dcca-45e4-9211-065abfc2b756.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2005.11401.pdf\n",
      "Enregistré sous: NLP-6/article_f8ecc57c-78f6-484a-8072-c072c14e9f9c.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.340.pdf\n",
      "Enregistré sous: NLP-6/article_5132eed7-f4e3-44a6-9871-6ff15e5e9ffa.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.168.pdf\n",
      "Enregistré sous: NLP-6/article_a20b36a0-fbdb-4b90-b4c9-4e25b6a3ad9a.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.442.pdf\n",
      "Enregistré sous: NLP-6/article_09f80e32-3b60-46ed-93b0-3c17461a1e91.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.findings-acl.84.pdf\n",
      "Enregistré sous: NLP-6/article_5e83a042-4e04-4067-a4d8-7fc88dbf04e1.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P19-1452.pdf\n",
      "Enregistré sous: NLP-6/article_21cc935d-ae5d-42cd-8c14-99379a96842f.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n",
      "Enregistré sous: NLP-6/article_7f7559a6-745a-4677-a578-aba7872d2ac0.pdf\n",
      "\n",
      "Page 2\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-6/article_fc2feb2a-eb9d-464c-b153-00e364b560c2.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n",
      "Enregistré sous: NLP-6/article_c67a663f-f7e9-4a28-addf-f3268731bb15.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.560.pdf\n",
      "Enregistré sous: NLP-6/article_c130644f-514e-4c9d-bba6-5fcb7d56a061.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.386.pdf\n",
      "Enregistré sous: NLP-6/article_bc4f889e-9fdd-45c5-ad71-957524c9fb96.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.insights-1.1.pdf\n",
      "Enregistré sous: NLP-6/article_dcb4bc61-36f3-4261-8067-0153c5dbc363.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.324.pdf\n",
      "Enregistré sous: NLP-6/article_a8249757-6333-4535-919a-506fe7359ef9.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.500.pdf\n",
      "Enregistré sous: NLP-6/article_6138bdc6-5e19-4840-a59f-3cc1ff59e8b7.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.482.pdf\n",
      "Enregistré sous: NLP-6/article_82e2888b-953e-4ff1-b521-3e68e32c32bb.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-6/article_ecbfb0fa-ff29-44fd-8057-0c12687e5b5f.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/N19-4010.pdf\n",
      "Enregistré sous: NLP-6/article_3ffa849b-9d2f-41a8-a9d5-50784dd9956d.pdf\n",
      "\n",
      "Page 3\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s42256-023-00729-y.pdf\n",
      "Enregistré sous: NLP-6/article_7d084084-7030-4693-8642-620b53d99fed.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-6/article_1e26a6ca-a058-4249-9872-323abca4e42d.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.acl-long.230.pdf\n",
      "Enregistré sous: NLP-6/article_df47782a-2928-4661-848b-a9a1d2282e56.pdf\n",
      "Téléchargement du PDF: https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf\n",
      "Enregistré sous: NLP-6/article_7fc5f34a-8d04-43f6-80c3-920dba41ea15.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2202.04824.pdf\n",
      "Enregistré sous: NLP-6/article_022411de-bfb6-45af-a08d-f40932dc3cca.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/D19-1221.pdf\n",
      "Enregistré sous: NLP-6/article_529d4f01-a3b2-4392-909e-4aae79a5b2ae.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.646.pdf\n",
      "Enregistré sous: NLP-6/article_e57221a1-b386-4aa4-90f6-8af3fc690cf2.pdf\n",
      "Téléchargement du PDF: https://discovery.ucl.ac.uk/id/eprint/10149021/1/bjophthalmol-2022-321141.R1_Proof_hi.pdf\n",
      "Enregistré sous: NLP-6/article_71848500-7e75-43de-a01d-09872b3072fc.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.45.pdf\n",
      "Enregistré sous: NLP-6/article_5a4e1215-2596-4213-9122-7cb271e4bad3.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.emnlp-main.572.pdf\n",
      "Enregistré sous: NLP-6/article_0b799a37-d655-4231-a8c7-009d1e8d4c32.pdf\n",
      "\n",
      "Page 4\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.aacl-main.55.pdf\n",
      "Enregistré sous: NLP-6/article_d5d7c528-01a1-447d-9eb0-08fbd2f4f663.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-6/article_93fcbdf7-6175-4c41-b3a3-70a1d4cfc7cd.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2209.11326.pdf\n",
      "Enregistré sous: NLP-6/article_2ca5cca7-3ebc-4dc4-8cee-4a3c321f5e89.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-6/article_4d34b4ca-5a8c-423d-b5cd-7c0962364df7.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.263.pdf\n",
      "Enregistré sous: NLP-6/article_9926b0e8-d147-4c15-bd9a-5c4344f990e6.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.emnlp-main.159.pdf\n",
      "Enregistré sous: NLP-6/article_7fc8661e-6a5a-4475-bcb8-87a625c690b1.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-demos.6.pdf\n",
      "Enregistré sous: NLP-6/article_8ad58b49-ea3c-4f91-80b5-17d55efc3bf9.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2103.06268.pdf\n",
      "Enregistré sous: NLP-6/article_e09331a5-7369-4aa3-b0da-1b10d6fcafb1.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-6/article_a14ca81a-0399-4073-8e85-5a5ceee7ec6f.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/P18-1079.pdf\n",
      "Enregistré sous: NLP-6/article_aadead1d-f5c7-4262-8d4f-11dd23b5a05d.pdf\n",
      "\n",
      "Page 5\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.13.pdf\n",
      "Enregistré sous: NLP-6/article_f85bd8e9-d072-44c7-968b-99c83fe87bdc.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.13.pdf\n",
      "Enregistré sous: NLP-6/article_9649ac44-e123-402d-9d4b-b362da3d0567.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.165.pdf\n",
      "Enregistré sous: NLP-6/article_cfb09d76-0061-46ff-9b1a-3bac7410213d.pdf\n",
      "Téléchargement du PDF: https://pdfs.semanticscholar.org/c698/6906d5b0ca40f3f2ecc45646bf0fb9587a3e.pdf\n",
      "Enregistré sous: NLP-6/article_d3252e33-2a17-41ce-82e2-f4cb14d361a1.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2023.tacl-1.12.pdf\n",
      "Enregistré sous: NLP-6/article_862a6c44-5514-4999-882a-c30a7e13df88.pdf\n",
      "Téléchargement du PDF: https://research.fb.com/wp-content/uploads/2021/09/Transformers4Rec-Bridging-the-Gap-between-NLP-and-Sequential-Session-Based-Recommendation-1.pdf\n",
      "Enregistré sous: NLP-6/article_a5644411-a0ee-4a2f-aa3d-62116d86b8ee.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2022.naacl-main.339.pdf\n",
      "Enregistré sous: NLP-6/article_78271ce2-e664-4db4-85d4-c2facf48116d.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.487.pdf\n",
      "Enregistré sous: NLP-6/article_e6a7741f-6bfc-43b3-8734-76ee07db5433.pdf\n",
      "Téléchargement du PDF: https://www.nature.com/articles/s41467-020-19266-y.pdf\n",
      "Enregistré sous: NLP-6/article_888df569-338b-478f-8263-f88ea8e9ab12.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.coling-main.66.pdf\n",
      "Enregistré sous: NLP-6/article_1bbae97f-c2d8-4405-b9a4-96979ccf2f0b.pdf\n",
      "\n",
      "Page 6\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2109.00544.pdf\n",
      "Enregistré sous: NLP-6/article_76c409a8-143c-49e9-836a-39646e2b711b.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2107.07170.pdf\n",
      "Enregistré sous: NLP-6/article_b02e1275-5a93-4dec-ad6b-a8a9d8867598.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.acl-long.149.pdf\n",
      "Enregistré sous: NLP-6/article_be9c43f5-f3f7-44bb-8de0-c8bde7571d4e.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2021.naacl-main.53.pdf\n",
      "Enregistré sous: NLP-6/article_93959859-0cef-4470-81c0-26b7e29ceeac.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.coling-main.603.pdf\n",
      "Enregistré sous: NLP-6/article_8b4339b5-f141-40d4-843e-33b2f7df2e42.pdf\n",
      "Téléchargement du PDF: https://www.aclweb.org/anthology/2020.acl-main.466.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enregistré sous: NLP-6/article_a161ae6a-69a3-44e2-b32a-b08b63088168.pdf\n",
      "Téléchargement du PDF: https://arxiv.org/pdf/2004.08900.pdf\n",
      "Enregistré sous: NLP-6/article_a1e2f0d8-cd05-4df0-b37b-70de713c2c62.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.acl-long.431.pdf\n",
      "Enregistré sous: NLP-6/article_1045f7e8-93a3-4cb3-8dcf-effb53488949.pdf\n",
      "Téléchargement du PDF: https://www.aclanthology.org/2021.acl-long.431.pdf\n",
      "Enregistré sous: NLP-6/article_231823ad-2159-408b-b475-936fe752dae2.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import uuid # Importer le module uuid pour la génération de noms de fichiers uniques\n",
    "import random as rd\n",
    "\n",
    "#user_agents = [\n",
    "#    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.1 Safari/537.36\",\n",
    "#    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.1 Safari/537.36\",\n",
    "#    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.1 Safari/537.36\",\n",
    "    # Ajoutez d'autres User Agents au besoin\n",
    "#]\n",
    "user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "        ]\n",
    "\n",
    "def extraire_mots_cles_fr(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en français.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    requete_utilisateur = requete.lower()\n",
    "    doc = nlp(requete_utilisateur)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" and token.dep_ not in [\"ROOT\",\"obj\"]) or \\\n",
    "           (token.pos_ == \"ADV\" ) or \\\n",
    "           (token.pos_ == \"PRON\" and token.dep_ != \"dep\") or \\\n",
    "           (token.dep_ == \"ROOT\" and token.pos_ not in [\"NOUN\",\"VERB\"]) or \\\n",
    "           (token.pos_ == \"ADJ\" ) or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ in [\"nmod\",\"punct\"]):\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def extraire_mots_cles_en(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en anglais.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(requete)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" ) or  (token.dep_ == \"punct\") or \\\n",
    "           (token.pos_ == \"ADV\") or (token.dep_ == \"pobj\") or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ != \"dep\") or \\\n",
    "           (token.pos_ == \"ADJ\" ) :\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Fonction pour détecter la langue d'un texte donné.\n",
    "    \n",
    "    Arguments :\n",
    "        - text : Le texte à analyser\n",
    "        \n",
    "    Returns :\n",
    "        - 'français' si la langue détectée est le français, sinon 'anglais'\n",
    "    \"\"\"\n",
    "    lang = detect(text)\n",
    "    if lang == 'fr':\n",
    "        return 'français'\n",
    "    else:\n",
    "        return 'anglais'\n",
    "\n",
    "def format_mot(motcle):\n",
    "    \"\"\"\n",
    "    Fonction pour formater un mot-clé pour une utilisation dans une URL de recherche.\n",
    "    \n",
    "    Arguments :\n",
    "        - motcle : Le mot-clé à formater\n",
    "        \n",
    "    Returns :\n",
    "        - format_motcle : Le mot-clé formaté pour une URL de recherche\n",
    "    \"\"\"\n",
    "    if ' ' in motcle:\n",
    "        format_motcle = motcle.replace(' ', '+')\n",
    "    else:\n",
    "        format_motcle = motcle\n",
    "    return format_motcle\n",
    "\n",
    "def download_pdf_and_save(pdf_url, folder_name):\n",
    "    \"\"\"\n",
    "    Fonction pour télécharger un fichier PDF depuis un URL et l'enregistrer dans un dossier spécifié.\n",
    "    \n",
    "    Arguments :\n",
    "        - pdf_url : L'URL du fichier PDF à télécharger\n",
    "        - folder_name : Le nom du dossier dans lequel enregistrer le fichier PDF\n",
    "        \n",
    "    Returns :\n",
    "        - file_path : Le chemin du fichier enregistré\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": rd.choice(user_agents)}\n",
    "        response = requests.get(pdf_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        file_name = f\"article_{uuid.uuid4()}.pdf\"\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return file_path\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale du programme.\n",
    "    \"\"\"\n",
    "    requete_utilisateur = input('Veuillez saisir votre requête : ')\n",
    "\n",
    "    langue = detect_language(requete_utilisateur)\n",
    "\n",
    "    if langue == 'français':\n",
    "        mots_cles_format = extraire_mots_cles_fr(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Français\")\n",
    "    else:\n",
    "        mots_cles_format = extraire_mots_cles_en(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Anglais\")\n",
    "\n",
    "    search_query = format_mot(mots_cles_format)\n",
    "\n",
    "    num_pages = int(input(\"Entrez le nombre de pages à parcourir : \"))\n",
    "    folder_name = f\"{search_query.replace('+', '-')}-{num_pages}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    for page_num in range(1, num_pages+1):\n",
    "        print(f\"\\nPage {page_num}\")\n",
    "        page_url = f\"https://www.semanticscholar.org/search?q={search_query}&sort=relevance&page={page_num}\"\n",
    "        driver.get(page_url)\n",
    "        time.sleep(5) \n",
    "\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        pdf_links = soup.select('a[href$=\".pdf\"]')\n",
    "\n",
    "        for link in pdf_links:\n",
    "            pdf_url = link[\"href\"]\n",
    "            print(\"Téléchargement du PDF:\", pdf_url)\n",
    "            file_path = download_pdf_and_save(pdf_url, folder_name)\n",
    "            if file_path:\n",
    "                print(f\"Enregistré sous: {file_path}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480c4c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier a été téléchargé et enregistré sous le nom: YACINE.PDF\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def telecharger_pdf(url, nom_fichier):\n",
    "    # Envoyer une requête GET pour récupérer le contenu du fichier\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Écrire le contenu de la réponse dans un fichier local\n",
    "        with open(nom_fichier, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Le fichier a été téléchargé et enregistré sous le nom:\", nom_fichier)\n",
    "    else:\n",
    "        print(\"Impossible de télécharger le fichier. Statut de la requête:\", response.status_code)\n",
    "\n",
    "# Lien du PDF\n",
    "url_pdf = \"https://research.fb.com/wp-content/uploads/2021/09/Transformers4Rec-Bridging-the-Gap-between-NLP-and-Sequential-Session-Based-Recommendation-1.pdf\"\n",
    "\n",
    "# Nom du fichier local\n",
    "nom_local = \"YACINE.PDF\"\n",
    "\n",
    "# Appel de la fonction pour télécharger le PDF\n",
    "telecharger_pdf(url_pdf, nom_local)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302c01b",
   "metadata": {},
   "source": [
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a36bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veuillez saisir votre requête : I'm looking for articles on NLP\n",
      "Langue détectée ==> Anglais\n",
      "Entrez le nombre de pages à parcourir : 5\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Nombre total d'articles téléchargés avec succès : 49\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import uuid # Importer le module uuid pour la génération de noms de fichiers uniques\n",
    "import random as rd\n",
    "\n",
    "# Liste des User Agents\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n",
    "]\n",
    "\n",
    "def extraire_mots_cles_fr(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en français.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    requete_utilisateur = requete.lower()\n",
    "    doc = nlp(requete_utilisateur)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" and token.dep_ not in [\"ROOT\",\"obj\"]) or \\\n",
    "           (token.pos_ == \"ADV\" ) or \\\n",
    "           (token.pos_ == \"PRON\" and token.dep_ != \"dep\") or \\\n",
    "           (token.dep_ == \"ROOT\" and token.pos_ not in [\"NOUN\",\"VERB\"]) or \\\n",
    "           (token.pos_ == \"ADJ\" ) or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ in [\"nmod\",\"punct\"]):\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def extraire_mots_cles_en(requete):\n",
    "    \"\"\"\n",
    "    Fonction pour extraire les mots-clés à partir de la requête de l'utilisateur en anglais.\n",
    "    \n",
    "    Arguments :\n",
    "        - requete : La requête de l'utilisateur\n",
    "        \n",
    "    Returns :\n",
    "        - mots_cles_format : Les mots-clés formatés sous forme de chaîne de caractères\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(requete)\n",
    "    mots_cles = []\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\" ) or  (token.dep_ == \"punct\") or \\\n",
    "           (token.pos_ == \"ADV\") or (token.dep_ == \"pobj\") or \\\n",
    "           (token.pos_ == \"PROPN\" and token.dep_ != \"dep\") or \\\n",
    "           (token.pos_ == \"ADJ\" ) :\n",
    "            if not token.is_stop and not token.is_punct and token.text.lower() not in ['article', 'articles']:\n",
    "                mots_cles.append(token.text)\n",
    "    mots_cles_format = ' '.join(mots_cles)\n",
    "    return mots_cles_format\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Fonction pour détecter la langue d'un texte donné.\n",
    "    \n",
    "    Arguments :\n",
    "        - text : Le texte à analyser\n",
    "        \n",
    "    Returns :\n",
    "        - 'français' si la langue détectée est le français, sinon 'anglais'\n",
    "    \"\"\"\n",
    "    lang = detect(text)\n",
    "    if lang == 'fr':\n",
    "        return 'français'\n",
    "    else:\n",
    "        return 'anglais'\n",
    "\n",
    "def format_mot(motcle):\n",
    "    \"\"\"\n",
    "    Fonction pour formater un mot-clé pour une utilisation dans une URL de recherche.\n",
    "    \n",
    "    Arguments :\n",
    "        - motcle : Le mot-clé à formater\n",
    "        \n",
    "    Returns :\n",
    "        - format_motcle : Le mot-clé formaté pour une URL de recherche\n",
    "    \"\"\"\n",
    "    if ' ' in motcle:\n",
    "        format_motcle = motcle.replace(' ', '+')\n",
    "    else:\n",
    "        format_motcle = motcle\n",
    "    return format_motcle\n",
    "\n",
    "def download_pdf_and_save(pdf_url, folder_name):\n",
    "    \"\"\"\n",
    "    Fonction pour télécharger un fichier PDF depuis un URL et l'enregistrer dans un dossier spécifié.\n",
    "    \n",
    "    Arguments :\n",
    "        - pdf_url : L'URL du fichier PDF à télécharger\n",
    "        - folder_name : Le nom du dossier dans lequel enregistrer le fichier PDF\n",
    "        \n",
    "    Returns :\n",
    "        - file_path : Le chemin du fichier enregistré\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": rd.choice(user_agents)}\n",
    "        response = requests.get(pdf_url, headers=headers, stream=True)\n",
    "        response.raise_for_status()\n",
    "        file_name = f\"article_{uuid.uuid4()}.pdf\"\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return file_path\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors du téléchargement du PDF {pdf_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale du programme.\n",
    "    \"\"\"\n",
    "    requete_utilisateur = input('Veuillez saisir votre requête : ')\n",
    "\n",
    "    langue = detect_language(requete_utilisateur)\n",
    "\n",
    "    if langue == 'français':\n",
    "        mots_cles_format = extraire_mots_cles_fr(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Français\")\n",
    "    else:\n",
    "        mots_cles_format = extraire_mots_cles_en(requete_utilisateur)\n",
    "        print(\"Langue détectée ==> Anglais\")\n",
    "\n",
    "    search_query = format_mot(mots_cles_format)\n",
    "\n",
    "    num_pages = int(input(\"Entrez le nombre de pages à parcourir : \"))\n",
    "    folder_name = f\"{search_query.replace('+', '-')}-{num_pages}\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    num_articles = 0  # Compteur d'articles téléchargés avec succès\n",
    "\n",
    "    for page_num in range(1, num_pages+1):\n",
    "        print(f\"Page {page_num}\")\n",
    "        page_url = f\"https://www.semanticscholar.org/search?q={search_query}&sort=relevance&page={page_num}\"\n",
    "        driver.get(page_url)\n",
    "        time.sleep(5) \n",
    "\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        pdf_links = soup.select('a[href$=\".pdf\"]')\n",
    "\n",
    "        for link in pdf_links:\n",
    "            pdf_url = link[\"href\"]\n",
    "            #print(\"Téléchargement du PDF:\", pdf_url)\n",
    "            file_path = download_pdf_and_save(pdf_url, folder_name)\n",
    "            if file_path:\n",
    "                num_articles += 1\n",
    "                #print(f\"Enregistré sous: {file_path}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(f\"Nombre total d'articles téléchargés avec succès : {num_articles}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9023b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
